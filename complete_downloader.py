"""
å®Œæ•´çš„è‡ªå‹•åŒ–ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³ç³»çµ±
é©ç”¨æ–¼I7-11ä»£ (8æ ¸16ç·šç¨‹)

ã€ç¨ç«‹å¯ç§»æ¤ç‰ˆæœ¬ã€‘
- ä½¿ç”¨ config.py é€²è¡Œé…ç½®
- åŒ…å«å®Œæ•´çš„ NAS ä¸Šå‚³å™¨
- å¯ç§»å‹•åˆ°ä»»ä½•æœ‰ Python å’Œ SRA Toolkit çš„ç’°å¢ƒ
"""

import json
import subprocess
import time
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import sys
import os

# å°å…¥é…ç½®å’Œå·¥å…·
try:
    from config import *
    from nas_uploader import NASUploader
except ImportError as e:
    print(f"âŒ å°å…¥å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿ config.py å’Œ nas_uploader.py åœ¨åŒä¸€ç›®éŒ„")
    sys.exit(1)

# ==================== å¾ config.py è®€å–é…ç½® ====================

# ä¸¦è¡Œè¨­ç½® (å¾ config.py è®€å–)
# MAX_WORKERS = 6
# FASTERQ_THREADS = 5
# ç¸½è§£å£“ç·šç¨‹: 6 Ã— 5 = 30ç·šç¨‹

# è·¯å¾‘è¨­ç½® (å¾ config.py è®€å–)
SRA_TEMP_DIR = Path(SRA_TEMP_DIR)
TMP_DIR = Path(FASTQ_TEMP_DIR)
FASTQ_OUTPUT_DIR = Path(FASTQ_OUTPUT_DIR)

# NASè¨­ç½® (å¾ config.py è®€å–)
NAS_CONFIG = {
    "host": NAS_HOST,
    "port": NAS_PORT,
    "username": NAS_USER,
    "password": NAS_PASS,
    "fastq_path": NAS_FASTQ_PATH,
    "sra_path": NAS_SRA_PATH,
}

# è¶…æ™‚è¨­ç½®
PREFETCH_TIMEOUT = 3600  # 1å°æ™‚
FASTERQ_TIMEOUT = 5400  # 1.5å°æ™‚
UPLOAD_TIMEOUT = 3600  # 1å°æ™‚

# ==================== é€²åº¦ç®¡ç† ====================
# æ³¨æ„: NASUploader å·²å¾ nas_uploader.py å°å…¥


class ProgressManager:
    def __init__(self, progress_file="download_progress.json"):
        self.progress_file = Path(progress_file)
        self.progress = self.load_progress()
        self.save_count = 0  # è¿½è¹¤ä¿å­˜æ¬¡æ•¸
        self.last_backup_time = None  # ä¸Šæ¬¡å‚™ä»½æ™‚é–“

    def load_progress(self):
        """è¼‰å…¥é€²åº¦æª”æ¡ˆ,è‡ªå‹•è™•ç†éŒ¯èª¤ä¸¦æ¢å¾©å‚™ä»½"""
        if self.progress_file.exists():
            try:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
                if self.progress_file.stat().st_size == 0:
                    print("âš ï¸  é€²åº¦æª”æ¡ˆç‚ºç©º,å˜—è©¦æ¢å¾©å‚™ä»½...")
                    return self._restore_from_backup()

                # å˜—è©¦è¼‰å…¥ JSON
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    return json.load(f)

            except json.JSONDecodeError as e:
                print(f"âš ï¸  é€²åº¦æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
                print("   å˜—è©¦æ¢å¾©å‚™ä»½...")
                return self._restore_from_backup()

            except Exception as e:
                print(f"âš ï¸  è¼‰å…¥é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
                print("   ä½¿ç”¨æ–°çš„é€²åº¦è¨˜éŒ„")

        return {"completed": [], "failed": [], "remaining": []}

    def _restore_from_backup(self):
        """å¾å‚™ä»½æª”æ¡ˆæ¢å¾©"""
        import glob

        # å°‹æ‰¾æœ€æ–°çš„å‚™ä»½æª”æ¡ˆ
        backup_pattern = str(
            self.progress_file.parent / "download_progress_backup_*.json"
        )
        backups = sorted(glob.glob(backup_pattern), reverse=True)

        if backups:
            latest_backup = backups[0]
            try:
                print(f"   æ‰¾åˆ°å‚™ä»½: {Path(latest_backup).name}")
                with open(latest_backup, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # æ¢å¾©å‚™ä»½åˆ°ä¸»æª”æ¡ˆ
                import shutil

                shutil.copy(latest_backup, self.progress_file)
                print(f"âœ… å·²æ¢å¾©å‚™ä»½!")
                return data

            except Exception as e:
                print(f"âŒ æ¢å¾©å‚™ä»½å¤±æ•—: {e}")
        else:
            print("   æ‰¾ä¸åˆ°å‚™ä»½æª”æ¡ˆ")

        return {"completed": [], "failed": [], "remaining": []}

    def save_progress(self):
        """å®‰å…¨åœ°å„²å­˜é€²åº¦æª”æ¡ˆ (å…ˆå¯«è‡¨æ™‚æª”,å†é‡å‘½å)"""
        import os
        from datetime import datetime

        try:
            # æ¯10æ¬¡ä¿å­˜æˆ–æ¯30åˆ†é˜å‰µå»ºä¸€å€‹å¸¶æ™‚é–“æˆ³çš„å‚™ä»½
            self.save_count += 1
            current_time = datetime.now()

            should_backup = False
            if self.save_count % 10 == 0:  # æ¯10æ¬¡ä¿å­˜
                should_backup = True
            elif (
                self.last_backup_time is None
                or (current_time - self.last_backup_time).total_seconds() > 1800
            ):  # 30åˆ†é˜
                should_backup = True

            if should_backup:
                backup_name = f"download_progress_backup_{current_time.strftime('%Y%m%d_%H%M%S')}.json"
                backup_path = self.progress_file.parent / backup_name
                try:
                    if self.progress_file.exists():
                        import shutil

                        shutil.copy2(self.progress_file, backup_path)
                        self.last_backup_time = current_time
                        print(f"ğŸ’¾ å·²å‰µå»ºå‚™ä»½: {backup_name}")
                except Exception as e:
                    print(f"âš ï¸  å‰µå»ºå‚™ä»½å¤±æ•— (ç¹¼çºŒåŸ·è¡Œ): {e}")

            # å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
            temp_file = self.progress_file.with_suffix(".tmp")
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)

            # é©—è­‰ JSON æ ¼å¼æ­£ç¢º
            with open(temp_file, "r", encoding="utf-8") as f:
                json.load(f)

            # åŸå­æ€§æ›¿æ› - Windows å®‰å…¨åšæ³•
            # å…ˆå‰µå»ºå‚™ä»½,å†æ›¿æ›,æœ€å¾Œåˆªé™¤å‚™ä»½
            backup_file = None
            try:
                if self.progress_file.exists():
                    # å‰µå»ºå‚™ä»½
                    backup_file = self.progress_file.with_suffix(".bak")
                    if backup_file.exists():
                        backup_file.unlink()
                    self.progress_file.rename(backup_file)

                # é‡å‘½åè‡¨æ™‚æª”æ¡ˆç‚ºæ­£å¼æª”æ¡ˆ
                temp_file.rename(self.progress_file)

                # æˆåŠŸå¾Œåˆªé™¤å‚™ä»½
                if backup_file and backup_file.exists():
                    backup_file.unlink()

            except Exception as e:
                # å¦‚æœæ›¿æ›å¤±æ•—,æ¢å¾©å‚™ä»½
                if backup_file and backup_file.exists():
                    if not self.progress_file.exists():
                        backup_file.rename(self.progress_file)
                        print(f"âš ï¸  å„²å­˜å¤±æ•—,å·²æ¢å¾©å‚™ä»½: {e}")
                raise

        except Exception as e:
            print(f"âš ï¸  å„²å­˜é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass

    def mark_completed(self, run_id):
        """æ¨™è¨˜ç‚ºå®Œæˆ"""
        if run_id not in self.progress["completed"]:
            self.progress["completed"].append(run_id)
            self.progress["completed"].sort()

        # å¾failedç§»é™¤
        self.progress["failed"] = [
            f
            for f in self.progress.get("failed", [])
            if (isinstance(f, dict) and f.get("run_id") != run_id) or f != run_id
        ]

        self.save_progress()

    def mark_failed(self, run_id, step, error):
        """æ¨™è¨˜ç‚ºå¤±æ•—"""
        failure_entry = {
            "run_id": run_id,
            "step": step,
            "error": str(error),
            "time": datetime.now().isoformat(),
        }

        # æ›´æ–°æˆ–æ–°å¢å¤±æ•—è¨˜éŒ„
        failed_list = self.progress.get("failed", [])
        if (
            isinstance(failed_list, list)
            and len(failed_list) > 0
            and isinstance(failed_list[0], dict)
        ):
            # ç§»é™¤èˆŠè¨˜éŒ„
            failed_list = [f for f in failed_list if f.get("run_id") != run_id]
        else:
            failed_list = []

        failed_list.append(failure_entry)
        self.progress["failed"] = failed_list

        self.save_progress()


# ==================== ä¸‹è¼‰å™¨ ====================


def get_nas_samples():
    """ç²å–NASä¸Šå·²æœ‰çš„æ¨£æœ¬"""
    uploader = NASUploader(
        NAS_CONFIG["host"],
        NAS_CONFIG["port"],
        NAS_CONFIG["username"],
        NAS_CONFIG["password"],
    )

    try:
        if not uploader.connect():
            print(f"âš ï¸ ç„¡æ³•é€£æ¥åˆ°NAS")
            return set()

        samples = set()
        try:
            files = uploader.sftp.listdir(NAS_CONFIG["fastq_path"])
            for f in files:
                if f.endswith(".fastq"):
                    sample = f.rsplit("_", 1)[0]
                    samples.add(sample)
        except:
            pass

        uploader.disconnect()
        return samples

    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥NAS: {e}")
        uploader.disconnect()
        return set()


def get_all_runs_from_file():
    """å¾runs.txtè®€å–æ‰€æœ‰æ¨£æœ¬ (SRR, ERR, DRR)"""
    runs_file = Path("runs.txt")
    if not runs_file.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ°runs.txt")
        return set()

    all_runs = set()
    with open(runs_file, "r") as f:
        for line in f:
            run_id = line.strip()
            # è™•ç†æ‰€æœ‰Runé¡å‹: SRR, ERR, DRR
            if run_id and (
                run_id.startswith("SRR")
                or run_id.startswith("ERR")
                or run_id.startswith("DRR")
            ):
                all_runs.add(run_id)

    return all_runs


def get_missing_samples():
    """ç²å–éœ€è¦ä¸‹è¼‰çš„æ¨£æœ¬æ¸…å–®ï¼ˆ606å€‹runs.txt - NASå·²æœ‰çš„ï¼‰"""
    # å¾runs.txtè®€å–æ‰€æœ‰SRRæ¨£æœ¬
    all_runs = get_all_runs_from_file()

    print(f"ğŸ“„ runs.txtä¸­çš„SRRæ¨£æœ¬: {len(all_runs)} å€‹")

    # ç²å–NASå·²æœ‰çš„
    print(f"ğŸ” æ­£åœ¨æª¢æŸ¥NASå·²æœ‰æ¨£æœ¬...")
    nas_samples = get_nas_samples()

    print(f"âœ… NASå·²æœ‰: {len(nas_samples)} å€‹")

    # è¨ˆç®—ç¼ºå°‘çš„
    missing = all_runs - nas_samples

    print(f"ğŸ“Š éœ€è¦ä¸‹è¼‰: {len(missing)} å€‹")

    return sorted(list(missing))


def download_sample(run_id, nas_uploader, progress_mgr):
    """ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³å–®å€‹æ¨£æœ¬"""
    print(f"\n{'='*70}")
    print(f"ğŸ”„ è™•ç†æ¨£æœ¬: {run_id}")
    print(f"{'='*70}")

    sra_file = SRA_TEMP_DIR / run_id / f"{run_id}.sra"
    fastq_1 = FASTQ_OUTPUT_DIR / f"{run_id}_1.fastq"
    fastq_2 = FASTQ_OUTPUT_DIR / f"{run_id}_2.fastq"

    try:
        # ==================== æ­¥é©Ÿ1: Prefetch ====================
        print(f"\n[1/5] ğŸ“¥ ä¸‹è¼‰SRA...")
        sra_file.parent.mkdir(parents=True, exist_ok=True)

        cmd = [
            PREFETCH_EXE,  # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾‘
            run_id,
            "--output-directory",
            str(SRA_TEMP_DIR),
            "--max-size",
            "100GB",
        ]

        start_time = time.time()
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=PREFETCH_TIMEOUT
        )
        elapsed = time.time() - start_time

        if result.returncode != 0:
            raise Exception(f"Prefetchå¤±æ•—: {result.stderr}")

        if not sra_file.exists():
            raise Exception(f"SRAæª”æ¡ˆä¸å­˜åœ¨: {sra_file}")

        sra_size = sra_file.stat().st_size / (1024**3)
        print(f"âœ… Prefetchå®Œæˆ ({elapsed:.1f}ç§’, {sra_size:.2f} GB)")

        # ==================== æ­¥é©Ÿ2: Fasterq-dump ====================
        print(f"\n[2/5] ğŸ”“ è§£å£“FASTQ...")
        FASTQ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        cmd = [
            FASTERQ_DUMP_EXE,  # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾‘
            str(sra_file),
            "-e",
            str(FASTERQ_THREADS),
            "-O",
            str(FASTQ_OUTPUT_DIR),
            "-t",
            str(TMP_DIR),
            "-f",
        ]

        start_time = time.time()
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=FASTERQ_TIMEOUT
        )
        elapsed = time.time() - start_time

        if result.returncode != 0:
            # å¦‚æœè§£å£“å¤±æ•—ï¼Œå¾ˆæœ‰å¯èƒ½æ˜¯SRAæª”æ¡ˆæå£ï¼Œåˆªé™¤å®ƒä»¥ä¾¿é‡è©¦
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    âš ï¸  åµæ¸¬åˆ°è§£å£“å¤±æ•—ï¼Œå·²åˆªé™¤æå£çš„SRAç›®éŒ„: {sra_file.parent}")
            raise Exception(f"Fasterq-dumpå¤±æ•—: {result.stderr}")

        # å¢åŠ å°å–®ç«¯(Single-End)å’Œé›™ç«¯(Paired-End)çš„æª¢æŸ¥
        is_paired = fastq_1.exists() and fastq_2.exists()
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹ FASTQ æª”æ¡ˆå­˜åœ¨
        # Note: This logic is simplified. A more robust check might be needed if single-end files don't follow {run_id}.fastq pattern
        if not is_paired and not next(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq"), None):
            # å¦‚æœSRAæª”æ¡ˆå­˜åœ¨ï¼Œå‰‡åˆªé™¤å®ƒï¼Œå› ç‚ºå®ƒå¯èƒ½å·²æå£
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    âš ï¸  è§£å£“å¾Œæœªç”Ÿæˆä»»ä½•FASTQæª”æ¡ˆï¼Œå·²åˆªé™¤å¯èƒ½æå£çš„SRAç›®éŒ„: {sra_file.parent}")
            raise Exception(f"FASTQæª”æ¡ˆä¸å®Œæ•´æˆ–æœªç”Ÿæˆ")

        fastq_files_to_upload = []
        if is_paired:
            fastq_files_to_upload.extend([fastq_1, fastq_2])
            total_size = (fastq_1.stat().st_size + fastq_2.stat().st_size) / (1024**3)
            print(f"âœ… è§£å£“å®Œæˆ (é›™ç«¯, {elapsed:.1f}ç§’, {total_size:.2f} GB)")
        else:
            # è™•ç†å–®ç«¯æƒ…æ³æˆ–æª”åä¸ç‚º _1/_2 çš„æƒ…æ³
            single_fastq = next(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq"), None)
            if single_fastq and single_fastq.exists():
                fastq_files_to_upload.append(single_fastq)
                total_size = single_fastq.stat().st_size / (1024**3)
                print(f"âœ… è§£å£“å®Œæˆ (å–®ç«¯, {elapsed:.1f}ç§’, {total_size:.2f} GB)")
            else:
                # This case should be caught by the check above, but as a fallback
                if sra_file.parent.exists():
                    shutil.rmtree(sra_file.parent)
                raise Exception("æ‰¾ä¸åˆ°è§£å£“å¾Œçš„FASTQæª”æ¡ˆï¼Œå·²æ¸…ç†SRAæª”æ¡ˆä»¥ä¾¿é‡è©¦")


        # ==================== æ­¥é©Ÿ3: ä¸Šå‚³FASTQåˆ°NAS ====================
        print(f"\n[3/5] ğŸ“¤ ä¸Šå‚³FASTQåˆ°NAS...")

        for fastq_file in fastq_files_to_upload:
            remote_path = f"{NAS_CONFIG['fastq_path']}/{fastq_file.name}"
            if not nas_uploader.upload_file(fastq_file, remote_path, "FASTQ"):
                raise Exception(f"FASTQä¸Šå‚³å¤±æ•—: {fastq_file.name}")

        # ==================== æ­¥é©Ÿ4: ä¸Šå‚³SRAåˆ°NAS ====================
        print(f"\n[4/5] ğŸ“¤ ä¸Šå‚³SRAåˆ°NAS...")

        sra_remote_dir = f"{NAS_CONFIG['sra_path']}/{run_id}"
        sra_remote_path = f"{sra_remote_dir}/{sra_file.name}"

        nas_uploader.create_remote_dir(sra_remote_dir)
        if not nas_uploader.upload_file(sra_file, sra_remote_path, "SRA"):
            raise Exception("SRAä¸Šå‚³å¤±æ•—")

        # ==================== æ­¥é©Ÿ5: æ¸…ç†æœ¬åœ°æª”æ¡ˆ ====================
        print(f"\n[5/5] ğŸ§¹ æ¸…ç†æœ¬åœ°æª”æ¡ˆ...")

        # åˆªé™¤FASTQ
        for f in fastq_files_to_upload:
            if f.exists():
                f.unlink()
                print(f"    âœ… å·²åˆªé™¤: {f.name}")

        # åˆªé™¤SRAç›®éŒ„
        if sra_file.parent.exists():
            shutil.rmtree(sra_file.parent)
            print(f"    âœ… å·²åˆªé™¤: {sra_file.parent}")

        # æ¨™è¨˜ç‚ºå®Œæˆ
        progress_mgr.mark_completed(run_id)

        print(f"\nâœ… æ¨£æœ¬å®Œæˆ: {run_id}")
        return True

    except Exception as e:
        print(f"\nâŒ æ¨£æœ¬å¤±æ•—: {run_id}")
        print(f"   éŒ¯èª¤: {e}")

        # æ¸…ç†å¤±æ•—çš„æª”æ¡ˆ
        try:
            for f in [fastq_1, fastq_2]:
                if f.exists():
                    f.unlink()
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
        except:
            pass

        # æ¨™è¨˜ç‚ºå¤±æ•—
        progress_mgr.mark_failed(run_id, "download_process", str(e))

        return False


# ==================== ä¸»ç¨‹åº ====================


def main():
    print("=" * 80)
    print("ğŸš€ è‡ªå‹•åŒ–ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³ç³»çµ±")
    print("=" * 80)
    print(f"\nç³»çµ±é…ç½®:")
    print(f"  CPUå„ªåŒ–: I7-11ä»£ (8æ ¸16ç·šç¨‹)")
    print(f"  ä¸¦è¡Œæ•¸: {MAX_WORKERS} å€‹æ¨£æœ¬åŒæ™‚è™•ç†")
    print(f"  æ¯å€‹æ¨£æœ¬è§£å£“ç·šç¨‹: {FASTERQ_THREADS}")
    print(f"  ç¸½è§£å£“ç·šç¨‹æ•¸: {MAX_WORKERS * FASTERQ_THREADS}")
    print(f"  ç³»çµ±é ç•™: 2ç·šç¨‹")

    # å‰µå»ºå¿…è¦ç›®éŒ„
    SRA_TEMP_DIR.mkdir(parents=True, exist_ok=True)
    TMP_DIR.mkdir(parents=True, exist_ok=True)
    FASTQ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # é€£æ¥NAS
    print(f"\nğŸ”Œ æ­£åœ¨é€£æ¥NAS...")
    nas_uploader = NASUploader(
        NAS_CONFIG["host"],
        NAS_CONFIG["port"],
        NAS_CONFIG["username"],
        NAS_CONFIG["password"],
    )

    if not nas_uploader.connect():
        print("âŒ NASé€£æ¥å¤±æ•—ï¼Œç¨‹åºçµ‚æ­¢")
        return

    print("âœ… NASé€£æ¥æˆåŠŸ")

    # åˆå§‹åŒ–é€²åº¦ç®¡ç†
    progress_mgr = ProgressManager()

    # ç²å–ç¼ºå°‘çš„æ¨£æœ¬
    print(f"\nğŸ” æ­£åœ¨æª¢æŸ¥ç¼ºå°‘çš„æ¨£æœ¬...")
    missing_samples = get_missing_samples()

    print(f"\nğŸ“Š çµ±è¨ˆ:")
    print(f"  éœ€è¦ä¸‹è¼‰: {len(missing_samples)} å€‹æ¨£æœ¬")
    print(f"  NASè·¯å¾‘:")
    print(f"    FASTQ: {NAS_CONFIG['fastq_path']}")
    print(f"    SRA: {NAS_CONFIG['sra_path']}")

    if not missing_samples:
        print("\nâœ… æ‰€æœ‰æ¨£æœ¬éƒ½å·²åœ¨NASä¸Šï¼")
        nas_uploader.disconnect()
        return

    # ç¢ºèªé–‹å§‹
    print(f"\n" + "=" * 80)
    print(f"æº–å‚™é–‹å§‹ä¸‹è¼‰ {len(missing_samples)} å€‹æ¨£æœ¬")
    print(f"é ä¼°æ™‚é–“: {len(missing_samples) / MAX_WORKERS * 0.5:.1f} - {len(missing_samples) / MAX_WORKERS * 1.5:.1f} å°æ™‚")
    print(f"=" * 80)

    # å¦‚æœåœ¨éäº’å‹•å¼ç’°å¢ƒä¸­ï¼ˆä¾‹å¦‚ Dockerï¼‰ï¼Œå‰‡è·³éç­‰å¾…
    if not sys.stdout.isatty() and os.environ.get("DEBIAN_FRONTEND") == "noninteractive":
        print("\nåœ¨éäº’å‹•å¼ç’°å¢ƒä¸­ï¼Œè‡ªå‹•é–‹å§‹...")
    else:
        input("\næŒ‰Enteré–‹å§‹ï¼Œæˆ–Ctrl+Cå–æ¶ˆ...")

    # é–‹å§‹è™•ç†
    start_time = time.time()
    success_count = 0
    fail_count = 0

    print(f"\nğŸš€ é–‹å§‹è™•ç†...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(download_sample, run_id, nas_uploader, progress_mgr): run_id
            for run_id in missing_samples
        }

        for future in as_completed(futures):
            run_id = futures[future]
            try:
                if future.result():
                    success_count += 1
                else:
                    fail_count += 1
            except Exception as e:
                print(f"âŒ åŸ·è¡ŒéŒ¯èª¤ {run_id}: {e}")
                fail_count += 1

            # é¡¯ç¤ºé€²åº¦
            total_processed = success_count + fail_count
            print(f"\n{'='*80}")
            print(
                f"ğŸ“Š é€²åº¦: {total_processed}/{len(missing_samples)} "
                f"(æˆåŠŸ: {success_count}, å¤±æ•—: {fail_count})"
            )
            print(f"{'='*80}\n")

    # å®Œæˆ
    elapsed = time.time() - start_time

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ä»»å‹™å®Œæˆ")
    print("=" * 80)
    print(f"ç¸½è€—æ™‚: {elapsed/3600:.2f} å°æ™‚")
    print(f"æˆåŠŸ: {success_count} å€‹")
    print(f"å¤±æ•—: {fail_count} å€‹")

    nas_uploader.disconnect()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
