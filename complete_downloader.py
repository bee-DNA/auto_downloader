"""
å®Œæ•´çš„è‡ªå‹•åŒ–ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³ç³»çµ±
é©ç”¨æ–¼I7-11ä»£ (8æ ¸16ç·šç¨‹)

ã€ç¨ç«‹å¯ç§»æ¤ç‰ˆæœ¬ã€‘
- ä½¿ç”¨ config.py é€²è¡Œé…ç½®
- åŒ…å«å®Œæ•´çš„ NAS ä¸Šå‚³å™¨
- å¯ç§»å‹•åˆ°ä»»ä½•æœ‰ Python å’Œ SRA Toolkit çš„ç’°å¢ƒ
"""

import json
import subprocess
import time
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import sys
import os

# å°å…¥é…ç½®å’Œå·¥å…·
try:
    from config import *
    from nas_uploader import NASUploader
except ImportError as e:
    print(f"âŒ å°å…¥å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿ config.py å’Œ nas_uploader.py åœ¨åŒä¸€ç›®éŒ„")
    sys.exit(1)

# ==================== å¾ config.py è®€å–é…ç½® ====================

# ä¸¦è¡Œè¨­ç½® (å¾ config.py è®€å–)
# MAX_WORKERS = 6
# FASTERQ_THREADS = 5
# ç¸½è§£å£“ç·šç¨‹: 6 Ã— 5 = 30ç·šç¨‹

# è·¯å¾‘è¨­ç½® (å¾ config.py è®€å–)
SRA_TEMP_DIR = Path(SRA_TEMP_DIR)
TMP_DIR = Path(FASTQ_TEMP_DIR)
FASTQ_OUTPUT_DIR = Path(FASTQ_OUTPUT_DIR)

# NASè¨­ç½® (å¾ config.py è®€å–)
NAS_CONFIG = {
    "host": NAS_HOST,
    "port": NAS_PORT,
    "username": NAS_USER,
    "password": NAS_PASS,
    "fastq_path": NAS_FASTQ_PATH,
    "sra_path": NAS_SRA_PATH,
}

# è¶…æ™‚è¨­ç½®
PREFETCH_TIMEOUT = 3600  # 1å°æ™‚
FASTERQ_TIMEOUT = 5400  # 1.5å°æ™‚
UPLOAD_TIMEOUT = 3600  # 1å°æ™‚

# ==================== é€²åº¦ç®¡ç† ====================
# æ³¨æ„: NASUploader å·²å¾ nas_uploader.py å°å…¥


class ProgressManager:
    def __init__(self, progress_file="download_progress.json"):
        self.progress_file = Path(progress_file)
        self.progress = self.load_progress()
        self.save_count = 0  # è¿½è¹¤ä¿å­˜æ¬¡æ•¸
        self.last_backup_time = None  # ä¸Šæ¬¡å‚™ä»½æ™‚é–“

    def load_progress(self):
        """è¼‰å…¥é€²åº¦æª”æ¡ˆ,è‡ªå‹•è™•ç†éŒ¯èª¤ä¸¦æ¢å¾©å‚™ä»½"""
        if self.progress_file.exists():
            try:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
                if self.progress_file.stat().st_size == 0:
                    print("âš ï¸  é€²åº¦æª”æ¡ˆç‚ºç©º,å˜—è©¦æ¢å¾©å‚™ä»½...")
                    return self._restore_from_backup()

                # å˜—è©¦è¼‰å…¥ JSON
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    return json.load(f)

            except json.JSONDecodeError as e:
                print(f"âš ï¸  é€²åº¦æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
                print("   å˜—è©¦æ¢å¾©å‚™ä»½...")
                return self._restore_from_backup()

            except Exception as e:
                print(f"âš ï¸  è¼‰å…¥é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
                print("   ä½¿ç”¨æ–°çš„é€²åº¦è¨˜éŒ„")

        return {"completed": [], "failed": [], "remaining": []}

    def _restore_from_backup(self):
        """å¾å‚™ä»½æª”æ¡ˆæ¢å¾©"""
        import glob

        # å°‹æ‰¾æœ€æ–°çš„å‚™ä»½æª”æ¡ˆ
        backup_pattern = str(
            self.progress_file.parent / "download_progress_backup_*.json"
        )
        backups = sorted(glob.glob(backup_pattern), reverse=True)

        if backups:
            latest_backup = backups[0]
            try:
                print(f"   æ‰¾åˆ°å‚™ä»½: {Path(latest_backup).name}")
                with open(latest_backup, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # æ¢å¾©å‚™ä»½åˆ°ä¸»æª”æ¡ˆ
                import shutil

                shutil.copy(latest_backup, self.progress_file)
                print(f"âœ… å·²æ¢å¾©å‚™ä»½!")
                return data

            except Exception as e:
                print(f"âŒ æ¢å¾©å‚™ä»½å¤±æ•—: {e}")
        else:
            print("   æ‰¾ä¸åˆ°å‚™ä»½æª”æ¡ˆ")

        return {"completed": [], "failed": [], "remaining": []}

    def save_progress(self):
        """å®‰å…¨åœ°å„²å­˜é€²åº¦æª”æ¡ˆ (å…ˆå¯«è‡¨æ™‚æª”,å†é‡å‘½å)"""
        import os
        from datetime import datetime

        try:
            # æ¯10æ¬¡ä¿å­˜æˆ–æ¯30åˆ†é˜å‰µå»ºä¸€å€‹å¸¶æ™‚é–“æˆ³çš„å‚™ä»½
            self.save_count += 1
            current_time = datetime.now()

            should_backup = False
            if self.save_count % 10 == 0:  # æ¯10æ¬¡ä¿å­˜
                should_backup = True
            elif (
                self.last_backup_time is None
                or (current_time - self.last_backup_time).total_seconds() > 1800
            ):  # 30åˆ†é˜
                should_backup = True

            if should_backup:
                backup_name = f"download_progress_backup_{current_time.strftime('%Y%m%d_%H%M%S')}.json"
                backup_path = self.progress_file.parent / backup_name
                try:
                    if self.progress_file.exists():
                        import shutil

                        shutil.copy2(self.progress_file, backup_path)
                        self.last_backup_time = current_time
                        print(f"ğŸ’¾ å·²å‰µå»ºå‚™ä»½: {backup_name}")
                except Exception as e:
                    print(f"âš ï¸  å‰µå»ºå‚™ä»½å¤±æ•— (ç¹¼çºŒåŸ·è¡Œ): {e}")

            # å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
            temp_file = self.progress_file.with_suffix(".tmp")
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)

            # é©—è­‰ JSON æ ¼å¼æ­£ç¢º
            with open(temp_file, "r", encoding="utf-8") as f:
                json.load(f)

            # åŸå­æ€§æ›¿æ› - Windows å®‰å…¨åšæ³•
            # å…ˆå‰µå»ºå‚™ä»½,å†æ›¿æ›,æœ€å¾Œåˆªé™¤å‚™ä»½
            backup_file = None
            try:
                if self.progress_file.exists():
                    # å‰µå»ºå‚™ä»½
                    backup_file = self.progress_file.with_suffix(".bak")
                    if backup_file.exists():
                        backup_file.unlink()
                    self.progress_file.rename(backup_file)

                # é‡å‘½åè‡¨æ™‚æª”æ¡ˆç‚ºæ­£å¼æª”æ¡ˆ
                temp_file.rename(self.progress_file)

                # æˆåŠŸå¾Œåˆªé™¤å‚™ä»½
                if backup_file and backup_file.exists():
                    backup_file.unlink()

            except Exception as e:
                # å¦‚æœæ›¿æ›å¤±æ•—,æ¢å¾©å‚™ä»½
                if backup_file and backup_file.exists():
                    if not self.progress_file.exists():
                        backup_file.rename(self.progress_file)
                        print(f"âš ï¸  å„²å­˜å¤±æ•—,å·²æ¢å¾©å‚™ä»½: {e}")
                raise

        except Exception as e:
            print(f"âš ï¸  å„²å­˜é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass

    def mark_completed(self, run_id):
        """æ¨™è¨˜ç‚ºå®Œæˆ"""
        if run_id not in self.progress["completed"]:
            self.progress["completed"].append(run_id)
            self.progress["completed"].sort()

        # å¾failedç§»é™¤
        self.progress["failed"] = [
            f
            for f in self.progress.get("failed", [])
            if (isinstance(f, dict) and f.get("run_id") != run_id) or f != run_id
        ]

        self.save_progress()

    def mark_failed(self, run_id, step, error):
        """æ¨™è¨˜ç‚ºå¤±æ•—"""
        failure_entry = {
            "run_id": run_id,
            "step": step,
            "error": str(error),
            "time": datetime.now().isoformat(),
        }

        # æ›´æ–°æˆ–æ–°å¢å¤±æ•—è¨˜éŒ„
        failed_list = self.progress.get("failed", [])
        if (
            isinstance(failed_list, list)
            and len(failed_list) > 0
            and isinstance(failed_list[0], dict)
        ):
            # ç§»é™¤èˆŠè¨˜éŒ„
            failed_list = [f for f in failed_list if f.get("run_id") != run_id]
        else:
            failed_list = []

        failed_list.append(failure_entry)
        self.progress["failed"] = failed_list

        self.save_progress()


# ==================== ä¸‹è¼‰å™¨ ====================


def get_nas_samples():
    """ç²å–NASä¸Šå·²æœ‰çš„æ¨£æœ¬"""
    uploader = NASUploader(
        NAS_CONFIG["host"],
        NAS_CONFIG["port"],
        NAS_CONFIG["username"],
        NAS_CONFIG["password"],
    )

    try:
        if not uploader.connect():
            print(f"âš ï¸ ç„¡æ³•é€£æ¥åˆ°NAS")
            return set()

        samples = set()
        try:
            files = uploader.sftp.listdir(NAS_CONFIG["fastq_path"])
            for f in files:
                if f.endswith(".fastq"):
                    sample = f.rsplit("_", 1)[0]
                    samples.add(sample)
        except:
            pass

        uploader.disconnect()
        return samples

    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥NAS: {e}")
        uploader.disconnect()
        return set()


def get_all_runs_from_file():
    """å¾runs.txtè®€å–æ‰€æœ‰æ¨£æœ¬ (SRR, ERR, DRR)"""
    runs_file = Path("runs.txt")
    if not runs_file.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ°runs.txt")
        return set()

    all_runs = set()
    with open(runs_file, "r") as f:
        for line in f:
            run_id = line.strip()
            # è™•ç†æ‰€æœ‰Runé¡å‹: SRR, ERR, DRR
            if run_id and (
                run_id.startswith("SRR")
                or run_id.startswith("ERR")
                or run_id.startswith("DRR")
            ):
                all_runs.add(run_id)

    return all_runs


def get_missing_samples():
    """ç²å–éœ€è¦ä¸‹è¼‰çš„æ¨£æœ¬æ¸…å–®ï¼ˆ606å€‹runs.txt - NASå·²æœ‰çš„ï¼‰"""
    # å¾runs.txtè®€å–æ‰€æœ‰SRRæ¨£æœ¬
    all_runs = get_all_runs_from_file()

    print(f"ğŸ“„ runs.txtä¸­çš„SRRæ¨£æœ¬: {len(all_runs)} å€‹")

    # ç²å–NASå·²æœ‰çš„
    print(f"ğŸ” æ­£åœ¨æª¢æŸ¥NASå·²æœ‰æ¨£æœ¬...")
    nas_samples = get_nas_samples()

    print(f"âœ… NASå·²æœ‰: {len(nas_samples)} å€‹")

    # è¨ˆç®—ç¼ºå°‘çš„
    missing = all_runs - nas_samples

    print(f"ğŸ“Š éœ€è¦ä¸‹è¼‰: {len(missing)} å€‹")

    return sorted(list(missing))


def download_sample(run_id, progress_mgr):
    """ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³å–®å€‹æ¨£æœ¬ (æ¯å€‹ç·šç¨‹ç¨ç«‹é€£æ¥NAS)"""
    print(f"\n{'='*70}")
    print(f"ğŸ”„ è™•ç†æ¨£æœ¬: {run_id}")
    print(f"{'='*70}")

    nas_uploader = NASUploader(
        NAS_CONFIG["host"],
        NAS_CONFIG["port"],
        NAS_CONFIG["username"],
        NAS_CONFIG["password"],
    )

    sra_file = SRA_TEMP_DIR / run_id / f"{run_id}.sra"
    fastq_1 = FASTQ_OUTPUT_DIR / f"{run_id}_1.fastq"
    fastq_2 = FASTQ_OUTPUT_DIR / f"{run_id}_2.fastq"

    try:
        # ==================== å»ºç«‹ç¨ç«‹çš„NASé€£æ¥ ====================
        if not nas_uploader.connect():
            raise Exception("NASé€£æ¥å¤±æ•—")
        print(f"    ğŸ”Œ æ¨£æœ¬ {run_id} çš„ç¨ç«‹NASé€£æ¥å·²å»ºç«‹")
        # ==================== æ­¥é©Ÿ1: Prefetch ====================
        print(f"\n[1/5] ğŸ“¥ ä¸‹è¼‰SRA...")
        
        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        import shutil as shutil_disk
        disk_usage = shutil_disk.disk_usage(str(SRA_TEMP_DIR))
        free_gb = disk_usage.free / (1024**3)
        print(f"    ğŸ’¾ å¯ç”¨ç£ç¢Ÿç©ºé–“: {free_gb:.2f} GB")
        
        # å¦‚æœç©ºé–“ä¸è¶³ï¼Œå˜—è©¦è‡ªå‹•æ¸…ç†
        if free_gb < 50:  # ä½æ–¼ 50GB æ™‚è­¦å‘Šä¸¦æ¸…ç†
            print(f"    âš ï¸  ç£ç¢Ÿç©ºé–“åä½ï¼Œæ¸…ç†æ®˜ç•™æª”æ¡ˆ...")
            
            # æ¸…ç†ç©ºè³‡æ–™å¤¾
            empty_count = 0
            for item in Path(SRA_TEMP_DIR).iterdir():
                if item.is_dir() and not any(item.iterdir()):
                    try:
                        item.rmdir()
                        empty_count += 1
                    except:
                        pass
            
            # æ¸…ç†è‡¨æ™‚æª”
            temp_count = 0
            for tmp_file in Path(SRA_TEMP_DIR).rglob("*.tmp"):
                try:
                    tmp_file.unlink()
                    temp_count += 1
                except:
                    pass
            
            if empty_count > 0 or temp_count > 0:
                # é‡æ–°æª¢æŸ¥ç©ºé–“
                disk_usage = shutil_disk.disk_usage(str(SRA_TEMP_DIR))
                free_gb = disk_usage.free / (1024**3)
                print(f"    ğŸ§¹ å·²æ¸…ç†: {empty_count} å€‹ç©ºè³‡æ–™å¤¾, {temp_count} å€‹è‡¨æ™‚æª”")
                print(f"    ğŸ’¾ æ¸…ç†å¾Œå¯ç”¨ç©ºé–“: {free_gb:.2f} GB")
        
        if free_gb < 10:
            raise Exception(f"ç£ç¢Ÿç©ºé–“ä¸è¶³: åƒ…å‰© {free_gb:.2f} GBï¼Œè«‹æ‰‹å‹•æ¸…ç†æˆ–å¢åŠ ç£ç¢Ÿç©ºé–“")
        
        # æ¸…ç†æ•´å€‹ SRA ç›®éŒ„ï¼ˆç¢ºä¿å®Œå…¨ä¹¾æ·¨çš„ç‹€æ…‹ï¼‰
        if sra_file.parent.exists():
            # å…ˆæ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆå’Œé–æª”
            for tmp_file in sra_file.parent.glob("*.tmp"):
                try:
                    tmp_file.unlink()
                    print(f"    ğŸ—‘ï¸  å·²åˆªé™¤è‡¨æ™‚æª”: {tmp_file.name}")
                except:
                    pass
            for lock_file in sra_file.parent.glob("*.lock"):
                try:
                    lock_file.unlink()
                    print(f"    ğŸ—‘ï¸  å·²åˆªé™¤é–æª”: {lock_file.name}")
                except:
                    pass
            
            # ç„¶å¾Œåˆªé™¤æ•´å€‹ç›®éŒ„
            shutil.rmtree(sra_file.parent)
            print(f"    ğŸ—‘ï¸  å·²åˆªé™¤èˆŠçš„ SRA ç›®éŒ„: {sra_file.parent}")
        
        # é‡æ–°å‰µå»ºä¹¾æ·¨çš„ç›®éŒ„
        sra_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ç¢ºèªç›®éŒ„å‰µå»ºæˆåŠŸ
        if not sra_file.parent.exists():
            raise Exception(f"ç„¡æ³•å‰µå»ºç›®éŒ„: {sra_file.parent}")
        cmd = [
            PREFETCH_EXE,  # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾‘
            run_id,
            "--output-directory",
            str(SRA_TEMP_DIR),
            "--max-size",
            "100GB",
            "--force", "all",  # å¼·åˆ¶é‡æ–°ä¸‹è¼‰ï¼Œé¿å…éƒ¨åˆ†ä¸‹è¼‰è¡çª
        ]

        start_time = time.time()
        print(f"    åŸ·è¡ŒæŒ‡ä»¤: {' '.join(cmd)}")  # é™¤éŒ¯ï¼šé¡¯ç¤ºå¯¦éš›åŸ·è¡Œçš„æŒ‡ä»¤
        
        # ç¢ºèªç›®éŒ„åœ¨åŸ·è¡Œå‰ä»ç„¶å­˜åœ¨
        if not sra_file.parent.exists():
            raise Exception(f"ç›®éŒ„åœ¨ prefetch åŸ·è¡Œå‰æ¶ˆå¤±: {sra_file.parent}")
        
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=PREFETCH_TIMEOUT
        )
        elapsed = time.time() - start_time
        
        # çµ¦æª”æ¡ˆç³»çµ±ä¸€é»æ™‚é–“åŒæ­¥ï¼ˆDocker volume å¯èƒ½éœ€è¦ï¼‰
        time.sleep(2)
        
        # ç¢ºèªç›®éŒ„åœ¨åŸ·è¡Œå¾Œä»ç„¶å­˜åœ¨
        if not sra_file.parent.exists():
            raise Exception(f"ç›®éŒ„åœ¨ prefetch åŸ·è¡Œå¾Œæ¶ˆå¤±: {sra_file.parent}ï¼ˆå¯èƒ½è¢«å…¶ä»–åŸ·è¡Œç·’åˆªé™¤ï¼‰")

        if result.returncode != 0:
            # è¼¸å‡ºå®Œæ•´éŒ¯èª¤è¨Šæ¯ä»¥ä¾¿é™¤éŒ¯
            print(f"    âŒ Prefetchè¿”å›ç¢¼: {result.returncode}")
            print(f"    ğŸ“‹ STDOUT: {result.stdout}")
            print(f"    ğŸ“‹ STDERR: {result.stderr}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºè·¯å¾‘å•é¡Œï¼ˆå¯èƒ½æ˜¯ä¸¦è¡Œè¡çªï¼‰
            error_msg = result.stderr.lower()
            if "path not found" in error_msg or "cannot openfilewrite" in error_msg:
                raise Exception(f"Prefetchè·¯å¾‘éŒ¯èª¤ï¼ˆå¯èƒ½æ˜¯ä¸¦è¡Œè¡çªæˆ–æ¬Šé™å•é¡Œï¼‰: {result.stderr}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨£æœ¬ä¸å­˜åœ¨çš„éŒ¯èª¤
            if "item not found" in error_msg or "cannot resolve" in error_msg:
                raise Exception(f"æ¨£æœ¬ä¸å­˜åœ¨æ–¼SRAæ•¸æ“šåº«ï¼ˆå¯èƒ½å·²ä¸‹æ¶ï¼‰: {run_id}")
            raise Exception(f"Prefetchå¤±æ•—: {result.stderr}")

        if not sra_file.exists():
            raise Exception(f"SRAæª”æ¡ˆä¸å­˜åœ¨: {sra_file}")

        sra_size = sra_file.stat().st_size / (1024**3)
        print(f"âœ… Prefetchå®Œæˆ ({elapsed:.1f}ç§’, {sra_size:.2f} GB)")

        # ==================== æ­¥é©Ÿ1.5: é©—è­‰SRAæª”æ¡ˆå®Œæ•´æ€§ ====================
        print(f"\n[1.5/5] ğŸ” é©—è­‰SRAæª”æ¡ˆå®Œæ•´æ€§...")
        
        cmd_validate = [
            VDB_VALIDATE_EXE,
            str(sra_file)
        ]
        
        start_time = time.time()
        result_validate = subprocess.run(
            cmd_validate, capture_output=True, text=True, timeout=1800  # 30åˆ†é˜è¶…æ™‚
        )
        elapsed_validate = time.time() - start_time
        
        if result_validate.returncode != 0:
            # æ ¡é©—å¤±æ•—ï¼Œè¡¨ç¤ºSRAæª”æ¡ˆä¸å®Œæ•´æˆ–æå£
            print(f"    âŒ SRAæª”æ¡ˆæ ¡é©—å¤±æ•— ({elapsed_validate:.1f}ç§’)")
            print(f"    éŒ¯èª¤è¨Šæ¯: {result_validate.stderr[:200]}")
            
            # åˆªé™¤æå£çš„SRAæª”æ¡ˆ
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    ğŸ—‘ï¸  å·²åˆªé™¤æå£çš„SRAæª”æ¡ˆ: {sra_file.parent}")
            
            raise Exception(f"SRAæª”æ¡ˆå®Œæ•´æ€§æ ¡é©—å¤±æ•—ï¼Œæª”æ¡ˆå¯èƒ½ä¸‹è¼‰ä¸å®Œæ•´ (å¯¦éš›å¤§å°: {sra_size:.2f} GB)")
        
        print(f"âœ… SRAæª”æ¡ˆæ ¡é©—é€šé ({elapsed_validate:.1f}ç§’)")

        # ==================== æ­¥é©Ÿ2: Fasterq-dump ====================
        print(f"\n[2/5] ğŸ”“ è§£å£“FASTQ...")
        FASTQ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        cmd = [
            FASTERQ_DUMP_EXE,  # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾‘
            str(sra_file),
            "-e",
            str(FASTERQ_THREADS),
            "-O",
            str(FASTQ_OUTPUT_DIR),
            "-t",
            str(TMP_DIR),
            "--split-files",  # åˆ†é›¢æˆ _1.fastq å’Œ _2.fastq
            "-f",
        ]

        start_time = time.time()
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=FASTERQ_TIMEOUT
        )
        elapsed = time.time() - start_time

        if result.returncode != 0:
            # å¦‚æœè§£å£“å¤±æ•—ï¼Œå¾ˆæœ‰å¯èƒ½æ˜¯SRAæª”æ¡ˆæå£ï¼Œåˆªé™¤å®ƒä»¥ä¾¿é‡è©¦
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    âš ï¸  åµæ¸¬åˆ°è§£å£“å¤±æ•—ï¼Œå·²åˆªé™¤æå£çš„SRAç›®éŒ„: {sra_file.parent}")
            raise Exception(f"Fasterq-dumpå¤±æ•—: {result.stderr}")

        # å¢åŠ å°å–®ç«¯(Single-End)å’Œé›™ç«¯(Paired-End)çš„æª¢æŸ¥
        is_paired = fastq_1.exists() and fastq_2.exists()
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹ FASTQ æª”æ¡ˆå­˜åœ¨
        # Note: This logic is simplified. A more robust check might be needed if single-end files don't follow {run_id}.fastq pattern
        if not is_paired and not next(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq"), None):
            # å¦‚æœSRAæª”æ¡ˆå­˜åœ¨ï¼Œå‰‡åˆªé™¤å®ƒï¼Œå› ç‚ºå®ƒå¯èƒ½å·²æå£
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    âš ï¸  è§£å£“å¾Œæœªç”Ÿæˆä»»ä½•FASTQæª”æ¡ˆï¼Œå·²åˆªé™¤å¯èƒ½æå£çš„SRAç›®éŒ„: {sra_file.parent}")
            raise Exception(f"FASTQæª”æ¡ˆä¸å®Œæ•´æˆ–æœªç”Ÿæˆ")

        fastq_files_to_upload = []
        if is_paired:
            fastq_files_to_upload.extend([fastq_1, fastq_2])
            total_size = (fastq_1.stat().st_size + fastq_2.stat().st_size) / (1024**3)
            print(f"âœ… è§£å£“å®Œæˆ (é›™ç«¯, {elapsed:.1f}ç§’, {total_size:.2f} GB)")
        else:
            # è™•ç†å–®ç«¯æƒ…æ³æˆ–æª”åä¸ç‚º _1/_2 çš„æƒ…æ³
            single_fastq = next(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq"), None)
            if single_fastq and single_fastq.exists():
                fastq_files_to_upload.append(single_fastq)
                total_size = single_fastq.stat().st_size / (1024**3)
                print(f"âœ… è§£å£“å®Œæˆ (å–®ç«¯, {elapsed:.1f}ç§’, {total_size:.2f} GB)")
            else:
                # This case should be caught by the check above, but as a fallback
                if sra_file.parent.exists():
                    shutil.rmtree(sra_file.parent)
                raise Exception("æ‰¾ä¸åˆ°è§£å£“å¾Œçš„FASTQæª”æ¡ˆï¼Œå·²æ¸…ç†SRAæª”æ¡ˆä»¥ä¾¿é‡è©¦")


        # ==================== æ­¥é©Ÿ3: ä¸Šå‚³FASTQåˆ°NAS ====================
        print(f"\n[3/5] ğŸ“¤ ä¸Šå‚³FASTQåˆ°NAS...")

        for fastq_file in fastq_files_to_upload:
            remote_path = f"{NAS_CONFIG['fastq_path']}/{fastq_file.name}"
            if not nas_uploader.upload_file(fastq_file, remote_path, show_progress=True):
                raise Exception(f"FASTQä¸Šå‚³å¤±æ•—: {fastq_file.name}")

        # ==================== æ­¥é©Ÿ4: ä¸Šå‚³SRAåˆ°NASï¼ˆå·²åœç”¨ï¼‰ ====================
        # è¨»è§£ï¼šç”±æ–¼ SRA æª”æ¡ˆä¸Šå‚³ç¶“å¸¸å¤±æ•—ä¸”ä¸æ˜¯å¿…éœ€çš„ï¼ˆFASTQ å·²è¶³å¤ ï¼‰ï¼Œå› æ­¤åœç”¨æ­¤æ­¥é©Ÿ
        # print(f"\n[4/5] ğŸ“¤ ä¸Šå‚³SRAåˆ°NAS...")
        # sra_remote_dir = f"{NAS_CONFIG['sra_path']}/{run_id}"
        # sra_remote_path = f"{sra_remote_dir}/{sra_file.name}"
        # nas_uploader.create_remote_dir(sra_remote_dir)
        # if not nas_uploader.upload_file(sra_file, sra_remote_path, show_progress=True):
        #     raise Exception("SRAä¸Šå‚³å¤±æ•—")
        
        print(f"\n[4/5] â­ï¸  è·³éSRAä¸Šå‚³ï¼ˆFASTQå·²è¶³å¤ ï¼‰")

        # ==================== æ­¥é©Ÿ5: æ¸…ç†æœ¬åœ°æª”æ¡ˆ ====================
        print(f"\n[5/5] ğŸ§¹ æ¸…ç†æœ¬åœ°æª”æ¡ˆ...")

        # åˆªé™¤FASTQ
        for f in fastq_files_to_upload:
            if f.exists():
                f.unlink()
                print(f"    âœ… å·²åˆªé™¤: {f.name}")

        # åˆªé™¤SRAç›®éŒ„
        if sra_file.parent.exists():
            shutil.rmtree(sra_file.parent)
            print(f"    âœ… å·²åˆªé™¤: {sra_file.parent}")

        # æ¨™è¨˜ç‚ºå®Œæˆ
        progress_mgr.mark_completed(run_id)

        print(f"\nâœ… æ¨£æœ¬å®Œæˆ: {run_id}")
        return True

    except Exception as e:
        print(f"\nâŒ æ¨£æœ¬å¤±æ•—: {run_id}")
        print(f"   éŒ¯èª¤: {e}")

        # æ¸…ç†å¤±æ•—çš„æª”æ¡ˆ
        try:
            # Re-define paths for cleanup in case of early failure
            fastq_1 = FASTQ_OUTPUT_DIR / f"{run_id}_1.fastq"
            fastq_2 = FASTQ_OUTPUT_DIR / f"{run_id}_2.fastq"
            sra_file_parent = SRA_TEMP_DIR / run_id

            # Clean up any partial fastq files
            for f in list(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq")):
                if f.exists():
                    f.unlink()
            
            # Clean up SRA directory
            if sra_file_parent.exists():
                shutil.rmtree(sra_file_parent)
        except Exception as cleanup_error:
            print(f"    âš ï¸ æ¸…ç†å¤±æ•—æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {cleanup_error}")

        # æ¨™è¨˜ç‚ºå¤±æ•—
        # Extract step from error message if possible
        error_str = str(e).lower()
        
        # å€åˆ†ä¸åŒé¡å‹çš„å¤±æ•—
        if "æ¨£æœ¬ä¸å­˜åœ¨" in error_str or "item not found" in error_str:
            step = "sample_not_found"  # æ¨£æœ¬åœ¨æ•¸æ“šåº«ä¸­ä¸å­˜åœ¨
        else:
            step = "unknown_process"
        if "prefetch" in error_str:
            step = "prefetch"
        elif "fasterq-dump" in error_str or "fastq" in error_str:
            step = "dumping"
        elif "upload" in error_str:
            step = "upload"
        elif "nas" in error_str:
            step = "nas_connect"
        
        progress_mgr.mark_failed(run_id, step, str(e))

        return False
    
    finally:
        # ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œéƒ½ç¢ºä¿æ–·é–‹NASé€£æ¥
        if nas_uploader and nas_uploader.sftp:
            nas_uploader.disconnect()
            print(f"    ğŸ”Œ æ¨£æœ¬ {run_id} çš„ç¨ç«‹NASé€£æ¥å·²é—œé–‰")


# ==================== ä¸»ç¨‹åº ====================


def main():
    print("=" * 80)
    print("ğŸš€ è‡ªå‹•åŒ–ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³ç³»çµ±")
    print("=" * 80)
    print(f"\nç³»çµ±é…ç½®:")
    print(f"  CPUå„ªåŒ–: I7-11ä»£ (8æ ¸16ç·šç¨‹)")
    print(f"  ä¸¦è¡Œæ•¸: {MAX_WORKERS} å€‹æ¨£æœ¬åŒæ™‚è™•ç†")
    print(f"  æ¯å€‹æ¨£æœ¬è§£å£“ç·šç¨‹: {FASTERQ_THREADS}")
    print(f"  ç¸½è§£å£“ç·šç¨‹æ•¸: {MAX_WORKERS * FASTERQ_THREADS}")
    print(f"  ç³»çµ±é ç•™: 2ç·šç¨‹")

    # å‰µå»ºå¿…è¦ç›®éŒ„
    SRA_TEMP_DIR.mkdir(parents=True, exist_ok=True)
    TMP_DIR.mkdir(parents=True, exist_ok=True)
    FASTQ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # ç§»é™¤ä¸»å‡½æ•¸ä¸­çš„NASé€£æ¥ï¼Œæ”¹ç‚ºåœ¨æ¯å€‹ç·šç¨‹ä¸­ç¨ç«‹å‰µå»º
    # print(f"\nğŸ”Œ æ­£åœ¨é€£æ¥NAS...")
    # nas_uploader = NASUploader(...)
    # if not nas_uploader.connect():
    #     print("âŒ NASé€£æ¥å¤±æ•—ï¼Œç¨‹åºçµ‚æ­¢")
    #     return
    # print("âœ… NASé€£æ¥æˆåŠŸ")

    # åˆå§‹åŒ–é€²åº¦ç®¡ç†
    progress_mgr = ProgressManager()

    # ç²å–ç¼ºå°‘çš„æ¨£æœ¬
    print(f"\nğŸ” æ­£åœ¨æª¢æŸ¥ç¼ºå°‘çš„æ¨£æœ¬...")
    missing_samples = get_missing_samples()

    print(f"\nğŸ“Š çµ±è¨ˆ:")
    print(f"  éœ€è¦ä¸‹è¼‰: {len(missing_samples)} å€‹æ¨£æœ¬")
    print(f"  NASè·¯å¾‘:")
    print(f"    FASTQ: {NAS_CONFIG['fastq_path']}")
    print(f"    SRA: {NAS_CONFIG['sra_path']}")

    if not missing_samples:
        print("\nâœ… æ‰€æœ‰æ¨£æœ¬éƒ½å·²åœ¨NASä¸Šï¼")
        # nas_uploader.disconnect() # No longer needed here
        return

    # ç¢ºèªé–‹å§‹
    print(f"\n" + "=" * 80)
    print(f"æº–å‚™é–‹å§‹ä¸‹è¼‰ {len(missing_samples)} å€‹æ¨£æœ¬")
    print(f"é ä¼°æ™‚é–“: {len(missing_samples) / MAX_WORKERS * 0.5:.1f} - {len(missing_samples) / MAX_WORKERS * 1.5:.1f} å°æ™‚")
    print(f"=" * 80)

    # å¦‚æœåœ¨éäº’å‹•å¼ç’°å¢ƒä¸­ï¼ˆä¾‹å¦‚ Dockerï¼‰ï¼Œå‰‡è·³éç­‰å¾…
    if not sys.stdout.isatty() and os.environ.get("DEBIAN_FRONTEND") == "noninteractive":
        print("\nåœ¨éäº’å‹•å¼ç’°å¢ƒä¸­ï¼Œè‡ªå‹•é–‹å§‹...")
    else:
        input("\næŒ‰Enteré–‹å§‹ï¼Œæˆ–Ctrl+Cå–æ¶ˆ...")

    # é–‹å§‹è™•ç†
    start_time = time.time()
    success_count = 0
    fail_count = 0

    print(f"\nğŸš€ é–‹å§‹è™•ç†...")

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # å°‡ progress_mgr å‚³éçµ¦æ¯å€‹ä»»å‹™ï¼Œä¸å†å‚³éå…±äº«çš„ nas_uploader
        futures = {
            executor.submit(download_sample, run_id, progress_mgr): run_id
            for run_id in missing_samples
        }

        for future in as_completed(futures):
            run_id = futures[future]
            try:
                if future.result():
                    success_count += 1
                else:
                    fail_count += 1
            except Exception as e:
                print(f"âŒ åŸ·è¡ŒéŒ¯èª¤ {run_id}: {e}")
                fail_count += 1

            # é¡¯ç¤ºé€²åº¦
            total_processed = success_count + fail_count
            print(f"\n{'='*80}")
            print(
                f"ğŸ“Š é€²åº¦: {total_processed}/{len(missing_samples)} "
                f"(æˆåŠŸ: {success_count}, å¤±æ•—: {fail_count})"
            )
            print(f"{'='*80}\n")

    # å®Œæˆ
    elapsed = time.time() - start_time

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ä»»å‹™å®Œæˆ")
    print("=" * 80)
    print(f"ç¸½è€—æ™‚: {elapsed/3600:.2f} å°æ™‚")
    print(f"æˆåŠŸ: {success_count} å€‹")
    print(f"å¤±æ•—: {fail_count} å€‹")

    # é¡¯ç¤ºä¸å­˜åœ¨çš„æ¨£æœ¬åˆ—è¡¨
    if fail_count > 0:
        failed_list = progress_mgr.progress.get("failed", [])
        not_found_samples = [
            f["run_id"] for f in failed_list 
            if f.get("step") == "sample_not_found"
        ]
        
        if not_found_samples:
            print(f"\nâš ï¸  ä»¥ä¸‹ {len(not_found_samples)} å€‹æ¨£æœ¬åœ¨SRAæ•¸æ“šåº«ä¸­ä¸å­˜åœ¨ï¼ˆå¯èƒ½å·²ä¸‹æ¶ï¼‰:")
            for sample in not_found_samples:
                print(f"   - {sample}")
            print(f"\n   é€™äº›æ¨£æœ¬å°‡è¢«è·³éï¼Œæ˜¯æ­£å¸¸ç¾è±¡ã€‚")
        
        # é¡¯ç¤ºå…¶ä»–çœŸæ­£çš„éŒ¯èª¤
        real_errors = [
            f for f in failed_list 
            if f.get("step") != "sample_not_found"
        ]
        
        if real_errors:
            print(f"\nâŒ ä»¥ä¸‹ {len(real_errors)} å€‹æ¨£æœ¬ç™¼ç”ŸçœŸæ­£çš„éŒ¯èª¤:")
            for error in real_errors[:10]:  # æœ€å¤šé¡¯ç¤º10å€‹
                print(f"   - {error['run_id']}: {error.get('step', 'unknown')}")
                print(f"     {error.get('error', '')[:100]}")

    # nas_uploader.disconnect() # No longer needed here


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
