"""
å®Œæ•´çš„è‡ªå‹•åŒ–ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³ç³»çµ±
é©ç”¨æ–¼I7-11ä»£ (8æ ¸16ç·šç¨‹)

ã€ç¨ç«‹å¯ç§»æ¤ç‰ˆæœ¬ã€‘
- ä½¿ç”¨ config.py é€²è¡Œé…ç½®
- åŒ…å«å®Œæ•´çš„ NAS ä¸Šå‚³å™¨
- å¯ç§»å‹•åˆ°ä»»ä½•æœ‰ Python å’Œ SRA Toolkit çš„ç’°å¢ƒ
"""

import json
import subprocess
import time
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import sys
import os
import threading

# å°å…¥é…ç½®å’Œå·¥å…·
try:
    from config import *
    from nas_uploader import NASUploader
    from tqdm import tqdm
except ImportError as e:
    print(f"âŒ å°å…¥å¤±æ•—: {e}")
    if "tqdm" in str(e):
        print("è«‹å®‰è£ tqdm: pip install tqdm")
    else:
        print("è«‹ç¢ºä¿ config.py å’Œ nas_uploader.py åœ¨åŒä¸€ç›®éŒ„")
    sys.exit(1)

# ==================== å¾ config.py è®€å–é…ç½® ====================

# ä¸¦è¡Œè¨­ç½® (å¾ config.py è®€å–)
# MAX_WORKERS = 6
# FASTERQ_THREADS = 5
# ç¸½è§£å£“ç·šç¨‹: 6 Ã— 5 = 30ç·šç¨‹

# è·¯å¾‘è¨­ç½® (å¾ config.py è®€å–)
SRA_TEMP_DIR = Path(SRA_TEMP_DIR)
TMP_DIR = Path(FASTQ_TEMP_DIR)
FASTQ_OUTPUT_DIR = Path(FASTQ_OUTPUT_DIR)

# NASè¨­ç½® (å¾ config.py è®€å–)
NAS_CONFIG = {
    "host": NAS_HOST,
    "port": NAS_PORT,
    "username": NAS_USER,
    "password": NAS_PASS,
    "fastq_path": NAS_FASTQ_PATH,
    "sra_path": NAS_SRA_PATH,
}

# è¶…æ™‚è¨­ç½®ï¼ˆå¯é€éç’°å¢ƒè®Šæ•¸èª¿æ•´ï¼‰
PREFETCH_TIMEOUT = int(os.environ.get("PREFETCH_TIMEOUT", 10800))  # 3å°æ™‚ï¼ˆå¤§æª”æ¡ˆéœ€è¦æ›´é•·æ™‚é–“ï¼‰
FASTERQ_TIMEOUT = int(os.environ.get("FASTERQ_TIMEOUT", 10800))  # 3å°æ™‚
UPLOAD_TIMEOUT = int(os.environ.get("UPLOAD_TIMEOUT", 7200))  # 2å°æ™‚

# ==================== é€²åº¦ç®¡ç† ====================
# æ³¨æ„: NASUploader å·²å¾ nas_uploader.py å°å…¥


class ProgressManager:
    def __init__(self, progress_file="download_progress.json"):
        self.progress_file = Path(progress_file)
        self.progress = self.load_progress()
        self.save_count = 0  # è¿½è¹¤ä¿å­˜æ¬¡æ•¸
        self.last_backup_time = None  # ä¸Šæ¬¡å‚™ä»½æ™‚é–“

    def load_progress(self):
        """è¼‰å…¥é€²åº¦æª”æ¡ˆ,è‡ªå‹•è™•ç†éŒ¯èª¤ä¸¦æ¢å¾©å‚™ä»½"""
        if self.progress_file.exists():
            try:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
                if self.progress_file.stat().st_size == 0:
                    print("âš ï¸  é€²åº¦æª”æ¡ˆç‚ºç©º,å˜—è©¦æ¢å¾©å‚™ä»½...")
                    return self._restore_from_backup()

                # å˜—è©¦è¼‰å…¥ JSON
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    return json.load(f)

            except json.JSONDecodeError as e:
                print(f"âš ï¸  é€²åº¦æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
                print("   å˜—è©¦æ¢å¾©å‚™ä»½...")
                return self._restore_from_backup()

            except Exception as e:
                print(f"âš ï¸  è¼‰å…¥é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
                print("   ä½¿ç”¨æ–°çš„é€²åº¦è¨˜éŒ„")

        return {"completed": [], "failed": [], "remaining": []}

    def _restore_from_backup(self):
        """å¾å‚™ä»½æª”æ¡ˆæ¢å¾©"""
        import glob

        # å°‹æ‰¾æœ€æ–°çš„å‚™ä»½æª”æ¡ˆ
        backup_pattern = str(
            self.progress_file.parent / "download_progress_backup_*.json"
        )
        backups = sorted(glob.glob(backup_pattern), reverse=True)

        if backups:
            latest_backup = backups[0]
            try:
                print(f"   æ‰¾åˆ°å‚™ä»½: {Path(latest_backup).name}")
                with open(latest_backup, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # æ¢å¾©å‚™ä»½åˆ°ä¸»æª”æ¡ˆ
                import shutil

                shutil.copy(latest_backup, self.progress_file)
                print(f"âœ… å·²æ¢å¾©å‚™ä»½!")
                return data

            except Exception as e:
                print(f"âŒ æ¢å¾©å‚™ä»½å¤±æ•—: {e}")
        else:
            print("   æ‰¾ä¸åˆ°å‚™ä»½æª”æ¡ˆ")

        return {"completed": [], "failed": [], "remaining": []}

    def save_progress(self):
        """å®‰å…¨åœ°å„²å­˜é€²åº¦æª”æ¡ˆ (å…ˆå¯«è‡¨æ™‚æª”,å†é‡å‘½å)"""
        import os
        from datetime import datetime

        try:
            # æ¯10æ¬¡ä¿å­˜æˆ–æ¯30åˆ†é˜å‰µå»ºä¸€å€‹å¸¶æ™‚é–“æˆ³çš„å‚™ä»½
            self.save_count += 1
            current_time = datetime.now()

            should_backup = False
            if self.save_count % 10 == 0:  # æ¯10æ¬¡ä¿å­˜
                should_backup = True
            elif (
                self.last_backup_time is None
                or (current_time - self.last_backup_time).total_seconds() > 1800
            ):  # 30åˆ†é˜
                should_backup = True

            if should_backup:
                backup_name = f"download_progress_backup_{current_time.strftime('%Y%m%d_%H%M%S')}.json"
                backup_path = self.progress_file.parent / backup_name
                try:
                    if self.progress_file.exists():
                        import shutil

                        shutil.copy2(self.progress_file, backup_path)
                        self.last_backup_time = current_time
                        print(f"ğŸ’¾ å·²å‰µå»ºå‚™ä»½: {backup_name}")
                except Exception as e:
                    print(f"âš ï¸  å‰µå»ºå‚™ä»½å¤±æ•— (ç¹¼çºŒåŸ·è¡Œ): {e}")

            # å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
            temp_file = self.progress_file.with_suffix(".tmp")
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)

            # é©—è­‰ JSON æ ¼å¼æ­£ç¢º
            with open(temp_file, "r", encoding="utf-8") as f:
                json.load(f)

            # åŸå­æ€§æ›¿æ› - Windows å®‰å…¨åšæ³•
            # å…ˆå‰µå»ºå‚™ä»½,å†æ›¿æ›,æœ€å¾Œåˆªé™¤å‚™ä»½
            backup_file = None
            try:
                if self.progress_file.exists():
                    # å‰µå»ºå‚™ä»½
                    backup_file = self.progress_file.with_suffix(".bak")
                    if backup_file.exists():
                        backup_file.unlink()
                    self.progress_file.rename(backup_file)

                # é‡å‘½åè‡¨æ™‚æª”æ¡ˆç‚ºæ­£å¼æª”æ¡ˆ
                temp_file.rename(self.progress_file)

                # æˆåŠŸå¾Œåˆªé™¤å‚™ä»½
                if backup_file and backup_file.exists():
                    backup_file.unlink()

            except Exception as e:
                # å¦‚æœæ›¿æ›å¤±æ•—,æ¢å¾©å‚™ä»½
                if backup_file and backup_file.exists():
                    if not self.progress_file.exists():
                        backup_file.rename(self.progress_file)
                        print(f"âš ï¸  å„²å­˜å¤±æ•—,å·²æ¢å¾©å‚™ä»½: {e}")
                raise

        except Exception as e:
            print(f"âš ï¸  å„²å­˜é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass

    def mark_completed(self, run_id):
        """æ¨™è¨˜ç‚ºå®Œæˆ"""
        if run_id not in self.progress["completed"]:
            self.progress["completed"].append(run_id)
            self.progress["completed"].sort()

        # å¾failedç§»é™¤
        self.progress["failed"] = [
            f
            for f in self.progress.get("failed", [])
            if (isinstance(f, dict) and f.get("run_id") != run_id) or f != run_id
        ]

        self.save_progress()

    def mark_failed(self, run_id, step, error):
        """æ¨™è¨˜ç‚ºå¤±æ•—"""
        failure_entry = {
            "run_id": run_id,
            "step": step,
            "error": str(error),
            "time": datetime.now().isoformat(),
        }

        # æ›´æ–°æˆ–æ–°å¢å¤±æ•—è¨˜éŒ„
        failed_list = self.progress.get("failed", [])
        if (
            isinstance(failed_list, list)
            and len(failed_list) > 0
            and isinstance(failed_list[0], dict)
        ):
            # ç§»é™¤èˆŠè¨˜éŒ„
            failed_list = [f for f in failed_list if f.get("run_id") != run_id]
        else:
            failed_list = []

        failed_list.append(failure_entry)
        self.progress["failed"] = failed_list

        self.save_progress()


# ==================== ä¸‹è¼‰å™¨ ====================


def get_nas_samples():
    """ç²å–NASä¸Šå·²æœ‰çš„æ¨£æœ¬"""
    uploader = NASUploader(
        NAS_CONFIG["host"],
        NAS_CONFIG["port"],
        NAS_CONFIG["username"],
        NAS_CONFIG["password"],
    )

    try:
        if not uploader.connect():
            print(f"âš ï¸ ç„¡æ³•é€£æ¥åˆ°NAS")
            return set()

        samples = set()
        try:
            files = uploader.sftp.listdir(NAS_CONFIG["fastq_path"])
            for f in files:
                if f.endswith(".fastq"):
                    sample = f.rsplit("_", 1)[0]
                    samples.add(sample)
        except:
            pass

        uploader.disconnect()
        return samples

    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥NAS: {e}")
        uploader.disconnect()
        return set()


def get_all_runs_from_file():
    """å¾runs.txtè®€å–æ‰€æœ‰æ¨£æœ¬ (SRR, ERR, DRR)"""
    # æ”¯æ´é€éç’°å¢ƒè®Šæ•¸æŒ‡å®šä¸åŒçš„ runs æª”æ¡ˆ
    runs_filename = os.environ.get("RUNS_FILE", "runs.txt")
    runs_file = Path(runs_filename)
    
    if not runs_file.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ° {runs_filename}")
        return set()

    all_runs = set()
    with open(runs_file, "r") as f:
        for line in f:
            run_id = line.strip()
            # è™•ç†æ‰€æœ‰Runé¡å‹: SRR, ERR, DRR
            if run_id and (
                run_id.startswith("SRR")
                or run_id.startswith("ERR")
                or run_id.startswith("DRR")
            ):
                all_runs.add(run_id)

    return all_runs


def get_missing_samples():
    """ç²å–éœ€è¦ä¸‹è¼‰çš„æ¨£æœ¬æ¸…å–®ï¼ˆ606å€‹runs.txt - NASå·²æœ‰çš„ - é€²åº¦æª”æ¡ˆå·²å®Œæˆçš„ï¼‰"""
    # å¾runs.txtè®€å–æ‰€æœ‰SRRæ¨£æœ¬
    all_runs = get_all_runs_from_file()

    print(f"ğŸ“„ runs.txtä¸­çš„SRRæ¨£æœ¬: {len(all_runs)} å€‹")

    # ç²å–é€²åº¦ç®¡ç†å™¨
    progress_mgr = ProgressManager()
    completed_from_progress = set(progress_mgr.progress.get("completed", []))
    
    print(f"ğŸ“‹ é€²åº¦æª”æ¡ˆè¨˜éŒ„å·²å®Œæˆ: {len(completed_from_progress)} å€‹")

    # ç²å–NASå·²æœ‰çš„
    print(f"ğŸ” æ­£åœ¨æª¢æŸ¥NASå·²æœ‰æ¨£æœ¬...")
    nas_samples = get_nas_samples()

    print(f"âœ… NASå·²æœ‰: {len(nas_samples)} å€‹")

    # åˆä½µå·²å®Œæˆçš„æ¨£æœ¬ï¼ˆNASä¸Šçš„ + é€²åº¦æª”æ¡ˆè¨˜éŒ„çš„ï¼‰
    completed_samples = nas_samples | completed_from_progress
    print(f"ğŸ“Š ç¸½å…±å·²å®Œæˆ: {len(completed_samples)} å€‹ï¼ˆNAS + é€²åº¦æª”æ¡ˆï¼‰")

    # è¨ˆç®—ç¼ºå°‘çš„
    missing = all_runs - completed_samples

    print(f"ğŸ“Š éœ€è¦ä¸‹è¼‰: {len(missing)} å€‹")

    return sorted(list(missing))


def download_sample(run_id, progress_mgr):
    """ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³å–®å€‹æ¨£æœ¬ (æ¯å€‹ç·šç¨‹ç¨ç«‹é€£æ¥NAS)"""
    print(f"\n{'='*70}")
    print(f"ğŸ”„ è™•ç†æ¨£æœ¬: {run_id}")
    print(f"{'='*70}")

    nas_uploader = NASUploader(
        NAS_CONFIG["host"],
        NAS_CONFIG["port"],
        NAS_CONFIG["username"],
        NAS_CONFIG["password"],
    )

    sra_file = SRA_TEMP_DIR / run_id / f"{run_id}.sra"
    fastq_1 = FASTQ_OUTPUT_DIR / f"{run_id}_1.fastq"
    fastq_2 = FASTQ_OUTPUT_DIR / f"{run_id}_2.fastq"

    try:
        # ==================== å»ºç«‹ç¨ç«‹çš„NASé€£æ¥ ====================
        if not nas_uploader.connect():
            raise Exception("NASé€£æ¥å¤±æ•—")
        print(f"    ğŸ”Œ æ¨£æœ¬ {run_id} çš„ç¨ç«‹NASé€£æ¥å·²å»ºç«‹")
        # ==================== æ­¥é©Ÿ1: ä¸‹è¼‰ SRA ====================
        print(f"\n[1/5] ğŸ“¥ ä¸‹è¼‰SRA...", flush=True)
        
        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        import shutil as shutil_disk
        disk_usage = shutil_disk.disk_usage(str(SRA_TEMP_DIR))
        free_gb = disk_usage.free / (1024**3)
        print(f"    ğŸ’¾ å¯ç”¨ç£ç¢Ÿç©ºé–“: {free_gb:.2f} GB", flush=True)
        
        # å¦‚æœç©ºé–“ä¸è¶³ï¼Œè­¦å‘Šä½†ä¸è‡ªå‹•æ¸…ç†å…¶ä»–æ¨£æœ¬çš„ç›®éŒ„ï¼ˆé¿å…ä¸¦è¡Œè¡çªï¼‰
        if free_gb < 50:
            print(f"    âš ï¸  ç£ç¢Ÿç©ºé–“åä½: {free_gb:.2f} GB")
            print(f"    æç¤º: å»ºè­°æ‰‹å‹•åŸ·è¡Œ cleanup_disk.py æ¸…ç†æ®˜ç•™æª”æ¡ˆ")
        
        if free_gb < 10:
            raise Exception(f"ç£ç¢Ÿç©ºé–“ä¸è¶³: åƒ…å‰© {free_gb:.2f} GBï¼Œè«‹æ‰‹å‹•æ¸…ç†æˆ–å¢åŠ ç£ç¢Ÿç©ºé–“")
        
        # åªæ¸…ç†ç•¶å‰æ¨£æœ¬çš„èˆŠç›®éŒ„ï¼ˆé¿å…èª¤åˆªå…¶ä»–åŸ·è¡Œç·’çš„ç›®éŒ„ï¼‰
        if sra_file.parent.exists():
            # å…ˆæ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆå’Œé–æª”
            for tmp_file in sra_file.parent.glob("*.tmp"):
                try:
                    tmp_file.unlink()
                    print(f"    ğŸ—‘ï¸  å·²åˆªé™¤è‡¨æ™‚æª”: {tmp_file.name}")
                except:
                    pass
            for lock_file in sra_file.parent.glob("*.lock"):
                try:
                    lock_file.unlink()
                    print(f"    ğŸ—‘ï¸  å·²åˆªé™¤é–æª”: {lock_file.name}")
                except:
                    pass
            
            # ç„¶å¾Œåˆªé™¤æ•´å€‹ç›®éŒ„
            shutil.rmtree(sra_file.parent)
            print(f"    ğŸ—‘ï¸  å·²åˆªé™¤èˆŠçš„ SRA ç›®éŒ„: {sra_file.parent}")
        
        # é‡æ–°å‰µå»ºä¹¾æ·¨çš„ç›®éŒ„
        sra_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ç¢ºèªç›®éŒ„å‰µå»ºæˆåŠŸ
        if not sra_file.parent.exists():
            raise Exception(f"ç„¡æ³•å‰µå»ºç›®éŒ„: {sra_file.parent}")
        
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ aria2 åŠ é€Ÿä¸‹è¼‰
        use_aria2 = USE_ARIA2 and shutil.which("aria2c") is not None
        start_time = time.time()
        result = None
        
        if use_aria2:
            print(f"    ğŸš€ ä½¿ç”¨ aria2 å¤šé€£æ¥åŠ é€Ÿä¸‹è¼‰ï¼ˆ{ARIA2_CONNECTIONS} é€£æ¥ï¼‰...")
            
            # æ§‹å»º SRA ä¸‹è¼‰ URLï¼ˆå˜—è©¦å¤šå€‹é¡åƒï¼‰
            prefix = run_id[:6]
            mirrors = [
                f"https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos4/sra-pub-run-28/{prefix}/{run_id}/{run_id}.sra",
                f"https://sra-download.ncbi.nlm.nih.gov/traces/sra68/SRZ/{prefix}/{run_id}/{run_id}.sra",
                f"https://sra-pub-run-odp.s3.amazonaws.com/sra/{run_id}/{run_id}",
            ]
            
            # å˜—è©¦æ¯å€‹é¡åƒç›´åˆ°æˆåŠŸ
            download_success = False
            for mirror_idx, url in enumerate(mirrors, 1):
                print(f"    ğŸŒ å˜—è©¦é¡åƒ {mirror_idx}/{len(mirrors)}")
                
                aria2_cmd = [
                    "aria2c",
                    f"--max-connection-per-server={ARIA2_CONNECTIONS}",
                    f"--split={ARIA2_CONNECTIONS}",
                    "--min-split-size=1M",
                    "--max-concurrent-downloads=1",
                    "--continue=true",
                    "--max-tries=5",
                    "--retry-wait=3",
                    "--timeout=60",
                    "--connect-timeout=30",
                    f"--dir={sra_file.parent}",
                    f"--out={sra_file.name}",
                    url
                ]
                
                try:
                    result = subprocess.run(
                        aria2_cmd, capture_output=True, text=True, timeout=PREFETCH_TIMEOUT
                    )
                    
                    if result.returncode == 0 and sra_file.exists():
                        download_success = True
                        print(f"    âœ… aria2 ä¸‹è¼‰æˆåŠŸï¼")
                        break
                    else:
                        print(f"    âš ï¸ é¡åƒ {mirror_idx} å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹...")
                        
                except subprocess.TimeoutExpired:
                    print(f"    âš ï¸ é¡åƒ {mirror_idx} è¶…æ™‚ï¼Œå˜—è©¦ä¸‹ä¸€å€‹...")
                except Exception as e:
                    print(f"    âš ï¸ é¡åƒ {mirror_idx} éŒ¯èª¤: {e}")
            
            # å¦‚æœ aria2 å…¨éƒ¨å¤±æ•—ï¼Œå›é€€åˆ° prefetch
            if not download_success:
                print(f"    âš ï¸ aria2 æ‰€æœ‰é¡åƒéƒ½å¤±æ•—ï¼Œå›é€€åˆ° prefetch...")
                use_aria2 = False
        
        # å¦‚æœä¸ä½¿ç”¨ aria2 æˆ– aria2 å¤±æ•—ï¼Œä½¿ç”¨å‚³çµ± prefetch
        if not use_aria2:
            # æ§‹å»º prefetch å‘½ä»¤
            cmd = [
                PREFETCH_EXE,
                run_id,
                "--output-directory",
                str(SRA_TEMP_DIR),
                "--max-size",
                "100GB",
                "--force", "all",
                "--type", "sra",
                "--progress",
            ]
            
            # å˜—è©¦å•Ÿç”¨ Aspera åŠ é€Ÿ
            use_aspera = os.environ.get("USE_ASPERA", "yes").lower() in ["yes", "true", "1"]
            if use_aspera:
                pass  # prefetch æœƒè‡ªå‹•åµæ¸¬ä¸¦ä½¿ç”¨ Aspera
            
            print(f"    åŸ·è¡ŒæŒ‡ä»¤: {' '.join(cmd)}")
            
            # åŠ å…¥é‡è©¦æ©Ÿåˆ¶ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
            max_retries = 3
            retry_delay = 10
            
            for attempt in range(1, max_retries + 1):
                try:
                    result = subprocess.run(
                        cmd, capture_output=True, text=True, timeout=PREFETCH_TIMEOUT
                    )
                    
                    # å¦‚æœæˆåŠŸæˆ–éç¶²è·¯éŒ¯èª¤ï¼Œè·³å‡ºé‡è©¦
                    if result.returncode == 0:
                        break
                        
                    # æª¢æŸ¥æ˜¯å¦ç‚ºç¶²è·¯/é€£æ¥éŒ¯èª¤
                    error_msg = result.stderr.lower()
                    is_network_error = any(keyword in error_msg for keyword in [
                        "connection failed", "timeout", "network", "failed to download"
                    ])
                    
                    if is_network_error and attempt < max_retries:
                        print(f"    âš ï¸ ç¶²è·¯éŒ¯èª¤ï¼Œ{retry_delay}ç§’å¾Œé‡è©¦ ({attempt}/{max_retries})...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        break
                        
                except subprocess.TimeoutExpired:
                    if attempt < max_retries:
                        print(f"    âš ï¸ è¶…æ™‚ï¼Œ{retry_delay}ç§’å¾Œé‡è©¦ ({attempt}/{max_retries})...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        raise
        
        elapsed = time.time() - start_time

        # çµ¦æª”æ¡ˆç³»çµ±ä¸€é»æ™‚é–“åŒæ­¥ï¼ˆDocker volume å¯èƒ½éœ€è¦ï¼‰
        time.sleep(2)

        # å¦‚æœæª”æ¡ˆé‚„æ²’æœ‰å‡ºç¾ï¼Œå˜—è©¦è¼ªè©¢ä¸¦æœå°‹å¯èƒ½çš„ä½ç½®ï¼ˆä¾‹å¦‚ NCBI cacheï¼‰
        file_exists = sra_file.exists()
        postcheck_wait = int(os.environ.get("PREFETCH_POSTCHECK_WAIT", 120))
        poll_interval = 2
        checked_alt = []

        if not file_exists and result.returncode == 0:
            print(f"    âš ï¸ Prefetch è¿”å›æˆåŠŸä½†æª”æ¡ˆå°šæœªå¯è¦‹ï¼Œé–‹å§‹è¼ªè©¢æœ€å¤š {postcheck_wait}s...")
            t0 = time.time()
            while time.time() - t0 < postcheck_wait:
                if sra_file.exists():
                    file_exists = True
                    break
                # æœå°‹åŒç›®éŒ„ä¸‹çš„å€™é¸æª”æ¡ˆ
                if sra_file.parent.exists():
                    for f in sra_file.parent.iterdir():
                        try:
                            if f.is_file():
                                name = f.name.lower()
                                # å¸¸è¦‹æƒ…æ³: æª”ååŒ…å« run id æˆ–å‰¯æª”åç‚º .sra
                                if run_id.lower() in name or name.endswith(".sra"):
                                    # å˜—è©¦å°‡å®ƒç§»åˆ°é æœŸä½ç½®ï¼ˆå¦‚æœä¸åŒï¼‰
                                    try:
                                        if f.resolve() != sra_file.resolve():
                                            print(f"    ğŸ” ç™¼ç¾å€™é¸æª”æ¡ˆï¼Œå˜—è©¦é‡å‘½å: {f} -> {sra_file}")
                                            f.rename(sra_file)
                                    except Exception:
                                        # è§£æè·¯å¾‘å¯èƒ½å¤±æ•—ï¼Œæ”¹ç”¨ copy
                                        try:
                                            shutil.copy2(f, sra_file)
                                        except Exception:
                                            pass
                                    file_exists = sra_file.exists()
                                    break
                        except Exception:
                            continue
                # æœå°‹ NCBI é è¨­å¿«å–ä½ç½®
                try:
                    home_cache = Path.home() / ".ncbi" / "public" / "sra"
                    if home_cache.exists() and home_cache not in checked_alt:
                        checked_alt.append(home_cache)
                        for f in home_cache.rglob("*"):
                            if f.is_file() and run_id.lower() in f.name.lower():
                                print(f"    ğŸ” åœ¨ NCBI cache ç™¼ç¾å€™é¸: {f}ï¼Œè¤‡è£½åˆ° {sra_file}")
                                try:
                                    shutil.copy2(f, sra_file)
                                except Exception:
                                    pass
                                file_exists = sra_file.exists()
                                break
                except Exception:
                    pass

                if file_exists:
                    break
                time.sleep(poll_interval)

        # æª¢æŸ¥ prefetch æ˜¯å¦æˆåŠŸï¼ˆæª¢æŸ¥æª”æ¡ˆè€Œéç›®éŒ„ï¼Œé¿å…èª¤åˆ¤ï¼‰
        if result.returncode != 0 or not file_exists:
            # è¼¸å‡ºå®Œæ•´éŒ¯èª¤è¨Šæ¯ä»¥ä¾¿é™¤éŒ¯
            print(f"    âŒ Prefetchè¿”å›ç¢¼: {result.returncode}")
            print(f"    ğŸ“‹ STDOUT: {result.stdout}")
            print(f"    ğŸ“‹ STDERR: {result.stderr}")
            print(f"    ğŸ“ æª”æ¡ˆå­˜åœ¨: {file_exists}")
            print(f"    ğŸ“‚ é æœŸè·¯å¾‘: {sra_file}")

            # åˆ—å‡ºå¯¦éš›ä¸‹è¼‰çš„å…§å®¹ï¼ˆå¦‚æœç›®éŒ„å­˜åœ¨ï¼‰
            if sra_file.parent.exists():
                actual_files = list(sra_file.parent.rglob("*"))
                print(f"    ğŸ“‚ å¯¦éš›æª”æ¡ˆ: {[str(f) for f in actual_files[:20]]}")

            # æª¢æŸ¥æ˜¯å¦ç‚ºè·¯å¾‘å•é¡Œï¼ˆå¯èƒ½æ˜¯ä¸¦è¡Œè¡çªï¼‰
            error_msg = result.stderr.lower()
            if "path not found" in error_msg or "cannot openfilewrite" in error_msg:
                raise Exception(f"Prefetchè·¯å¾‘éŒ¯èª¤ï¼ˆå¯èƒ½æ˜¯ä¸¦è¡Œè¡çªæˆ–æ¬Šé™å•é¡Œï¼‰: {result.stderr}")

            # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨£æœ¬ä¸å­˜åœ¨çš„éŒ¯èª¤
            if "item not found" in error_msg or "cannot resolve" in error_msg:
                raise Exception(f"æ¨£æœ¬ä¸å­˜åœ¨æ–¼SRAæ•¸æ“šåº«ï¼ˆå¯èƒ½å·²ä¸‹æ¶ï¼‰: {run_id}")

            # æª¢æŸ¥æ˜¯å¦åƒ…ç‚ºåƒè€ƒåºåˆ—ä¸‹è¼‰å¤±æ•—ï¼ˆä½† SRA æœ¬èº«å·²ä¸‹è¼‰æˆåŠŸï¼‰
            is_refseq_only_error = (
                "failed to download" in error_msg and 
                "refseq" in error_msg and 
                sra_file.exists()
            )
            
            if is_refseq_only_error:
                print(f"    âš ï¸ åƒè€ƒåºåˆ—ä¸‹è¼‰å¤±æ•—ï¼Œä½† SRA æœ¬èº«å·²ä¸‹è¼‰å®Œæˆï¼Œç¹¼çºŒè™•ç†...")
                # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œè®“æµç¨‹ç¹¼çºŒ
            elif result.returncode == 0 and not file_exists:
                # å¦‚æœè¿”å›ç¢¼æ˜¯0ä½†æª”æ¡ˆä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–æª”æ¡ˆæ ¼å¼å•é¡Œ
                raise Exception(f"Prefetché¡¯ç¤ºæˆåŠŸä½†æª”æ¡ˆä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯ç¶²è·¯ä¸­æ–·æˆ–æ ¼å¼éŒ¯èª¤ï¼‰ã€‚STDOUT: {result.stdout[:200]}")
            else:
                raise Exception(f"Prefetchå¤±æ•—: {result.stderr}")

        if not sra_file.exists():
            raise Exception(f"SRAæª”æ¡ˆä¸å­˜åœ¨: {sra_file}")

        sra_size = sra_file.stat().st_size / (1024**3)
        print(f"âœ… Prefetchå®Œæˆ ({elapsed:.1f}ç§’, {sra_size:.2f} GB)")

        # ==================== æ­¥é©Ÿ1.5: é©—è­‰SRAæª”æ¡ˆå®Œæ•´æ€§ ====================
        print(f"\n[1.5/5] ğŸ” é©—è­‰SRAæª”æ¡ˆå®Œæ•´æ€§...", flush=True)
        
        cmd_validate = [
            VDB_VALIDATE_EXE,
            str(sra_file)
        ]
        
        start_time = time.time()
        result_validate = subprocess.run(
            cmd_validate, capture_output=True, text=True, timeout=1800  # 30åˆ†é˜è¶…æ™‚
        )
        elapsed_validate = time.time() - start_time
        
        if result_validate.returncode != 0:
            # æ ¡é©—å¤±æ•—ï¼Œè¡¨ç¤ºSRAæª”æ¡ˆä¸å®Œæ•´æˆ–æå£
            print(f"    âŒ SRAæª”æ¡ˆæ ¡é©—å¤±æ•— ({elapsed_validate:.1f}ç§’)")
            print(f"    éŒ¯èª¤è¨Šæ¯: {result_validate.stderr[:200]}")
            
            # åˆªé™¤æå£çš„SRAæª”æ¡ˆ
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    ğŸ—‘ï¸  å·²åˆªé™¤æå£çš„SRAæª”æ¡ˆ: {sra_file.parent}")
            
            raise Exception(f"SRAæª”æ¡ˆå®Œæ•´æ€§æ ¡é©—å¤±æ•—ï¼Œæª”æ¡ˆå¯èƒ½ä¸‹è¼‰ä¸å®Œæ•´ (å¯¦éš›å¤§å°: {sra_size:.2f} GB)")
        
        print(f"âœ… SRAæª”æ¡ˆæ ¡é©—é€šé ({elapsed_validate:.1f}ç§’)")

        # ==================== æ­¥é©Ÿ2: Fasterq-dump ====================
        print(f"\n[2/5] ğŸ”“ è§£å£“FASTQ...", flush=True)
        FASTQ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        cmd = [
            FASTERQ_DUMP_EXE,  # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾‘
            str(sra_file),
            "-e",
            str(FASTERQ_THREADS),
            "-O",
            str(FASTQ_OUTPUT_DIR),
            "-t",
            str(TMP_DIR),
            "--split-files",  # åˆ†é›¢æˆ _1.fastq å’Œ _2.fastq
            "-f",
        ]

        start_time = time.time()
        result = subprocess.run(
            cmd, capture_output=True, text=True, timeout=FASTERQ_TIMEOUT
        )
        elapsed = time.time() - start_time

        if result.returncode != 0:
            # å¦‚æœè§£å£“å¤±æ•—ï¼Œå¾ˆæœ‰å¯èƒ½æ˜¯SRAæª”æ¡ˆæå£ï¼Œåˆªé™¤å®ƒä»¥ä¾¿é‡è©¦
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    âš ï¸  åµæ¸¬åˆ°è§£å£“å¤±æ•—ï¼Œå·²åˆªé™¤æå£çš„SRAç›®éŒ„: {sra_file.parent}")
            raise Exception(f"Fasterq-dumpå¤±æ•—: {result.stderr}")

        # å¢åŠ å°å–®ç«¯(Single-End)å’Œé›™ç«¯(Paired-End)çš„æª¢æŸ¥
        is_paired = fastq_1.exists() and fastq_2.exists()
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹ FASTQ æª”æ¡ˆå­˜åœ¨
        # Note: This logic is simplified. A more robust check might be needed if single-end files don't follow {run_id}.fastq pattern
        if not is_paired and not next(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq"), None):
            # å¦‚æœSRAæª”æ¡ˆå­˜åœ¨ï¼Œå‰‡åˆªé™¤å®ƒï¼Œå› ç‚ºå®ƒå¯èƒ½å·²æå£
            if sra_file.parent.exists():
                shutil.rmtree(sra_file.parent)
                print(f"    âš ï¸  è§£å£“å¾Œæœªç”Ÿæˆä»»ä½•FASTQæª”æ¡ˆï¼Œå·²åˆªé™¤å¯èƒ½æå£çš„SRAç›®éŒ„: {sra_file.parent}")
            raise Exception(f"FASTQæª”æ¡ˆä¸å®Œæ•´æˆ–æœªç”Ÿæˆ")

        fastq_files_to_upload = []
        if is_paired:
            fastq_files_to_upload.extend([fastq_1, fastq_2])
            total_size = (fastq_1.stat().st_size + fastq_2.stat().st_size) / (1024**3)
            print(f"âœ… è§£å£“å®Œæˆ (é›™ç«¯, {elapsed:.1f}ç§’, {total_size:.2f} GB)")
        else:
            # è™•ç†å–®ç«¯æƒ…æ³æˆ–æª”åä¸ç‚º _1/_2 çš„æƒ…æ³
            single_fastq = next(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq"), None)
            if single_fastq and single_fastq.exists():
                fastq_files_to_upload.append(single_fastq)
                total_size = single_fastq.stat().st_size / (1024**3)
                print(f"âœ… è§£å£“å®Œæˆ (å–®ç«¯, {elapsed:.1f}ç§’, {total_size:.2f} GB)")
            else:
                # This case should be caught by the check above, but as a fallback
                if sra_file.parent.exists():
                    shutil.rmtree(sra_file.parent)
                raise Exception("æ‰¾ä¸åˆ°è§£å£“å¾Œçš„FASTQæª”æ¡ˆï¼Œå·²æ¸…ç†SRAæª”æ¡ˆä»¥ä¾¿é‡è©¦")

        # ==================== æ­¥é©Ÿ2.5: ç«‹å³åˆªé™¤SRAæª”æ¡ˆé‡‹æ”¾ç©ºé–“ ====================
        print(f"\n[2.5/5] ğŸ—‘ï¸  åˆªé™¤SRAæª”æ¡ˆé‡‹æ”¾ç©ºé–“...", flush=True)
        if sra_file.parent.exists():
            sra_size_gb = sra_file.stat().st_size / (1024**3) if sra_file.exists() else 0
            shutil.rmtree(sra_file.parent)
            print(f"    âœ… å·²åˆªé™¤SRAæª”æ¡ˆ (é‡‹æ”¾ {sra_size_gb:.2f} GB): {sra_file.parent}")

        # ==================== æ­¥é©Ÿ3: ä¸Šå‚³FASTQåˆ°NAS ====================
        print(f"\n[3/5] ğŸ“¤ ä¸Šå‚³FASTQåˆ°NAS...", flush=True)

        for fastq_file in fastq_files_to_upload:
            remote_path = f"{NAS_CONFIG['fastq_path']}/{fastq_file.name}"
            if not nas_uploader.upload_file(fastq_file, remote_path, show_progress=True):
                raise Exception(f"FASTQä¸Šå‚³å¤±æ•—: {fastq_file.name}")

        # ==================== æ­¥é©Ÿ4: ä¸Šå‚³SRAåˆ°NASï¼ˆå·²åœç”¨ï¼‰ ====================
        # è¨»è§£ï¼šç”±æ–¼ SRA æª”æ¡ˆä¸Šå‚³ç¶“å¸¸å¤±æ•—ä¸”ä¸æ˜¯å¿…éœ€çš„ï¼ˆFASTQ å·²è¶³å¤ ï¼‰ï¼Œå› æ­¤åœç”¨æ­¤æ­¥é©Ÿ
        # print(f"\n[4/5] ğŸ“¤ ä¸Šå‚³SRAåˆ°NAS...")
        # sra_remote_dir = f"{NAS_CONFIG['sra_path']}/{run_id}"
        # sra_remote_path = f"{sra_remote_dir}/{sra_file.name}"
        # nas_uploader.create_remote_dir(sra_remote_dir)
        # if not nas_uploader.upload_file(sra_file, sra_remote_path, show_progress=True):
        #     raise Exception("SRAä¸Šå‚³å¤±æ•—")
        
        print(f"\n[4/5] â­ï¸  è·³éSRAä¸Šå‚³ï¼ˆFASTQå·²è¶³å¤ ï¼‰", flush=True)

        # ==================== æ­¥é©Ÿ5: æ¸…ç†æœ¬åœ°æª”æ¡ˆ ====================
        print(f"\n[5/5] ğŸ§¹ æ¸…ç†æœ¬åœ°æª”æ¡ˆ...", flush=True)

        # åˆªé™¤FASTQ
        for f in fastq_files_to_upload:
            if f.exists():
                f.unlink()
                print(f"    âœ… å·²åˆªé™¤: {f.name}")

        # SRA å·²åœ¨æ­¥é©Ÿ 2.5 åˆªé™¤ï¼Œé€™è£¡ä¸éœ€è¦å†åˆªé™¤
        # ä½†ä¿ç•™æª¢æŸ¥ä»¥é˜²è¬ä¸€
        if sra_file.parent.exists():
            shutil.rmtree(sra_file.parent)
            print(f"    âš ï¸  ç™¼ç¾æ®˜ç•™çš„SRAç›®éŒ„ï¼Œå·²åˆªé™¤: {sra_file.parent}")

        # æ¨™è¨˜ç‚ºå®Œæˆ
        progress_mgr.mark_completed(run_id)

        print(f"\nâœ… æ¨£æœ¬å®Œæˆ: {run_id}")
        return True

    except Exception as e:
        print(f"\nâŒ æ¨£æœ¬å¤±æ•—: {run_id}")
        print(f"   éŒ¯èª¤: {e}")

        # æ¸…ç†å¤±æ•—çš„æª”æ¡ˆ
        try:
            # Re-define paths for cleanup in case of early failure
            fastq_1 = FASTQ_OUTPUT_DIR / f"{run_id}_1.fastq"
            fastq_2 = FASTQ_OUTPUT_DIR / f"{run_id}_2.fastq"
            sra_file_parent = SRA_TEMP_DIR / run_id

            # Clean up any partial fastq files
            for f in list(FASTQ_OUTPUT_DIR.glob(f"{run_id}*.fastq")):
                if f.exists():
                    f.unlink()
            
            # Clean up SRA directory
            if sra_file_parent.exists():
                shutil.rmtree(sra_file_parent)
        except Exception as cleanup_error:
            print(f"    âš ï¸ æ¸…ç†å¤±æ•—æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {cleanup_error}")

        # æ¨™è¨˜ç‚ºå¤±æ•—
        # Extract step from error message if possible
        error_str = str(e).lower()
        
        # å€åˆ†ä¸åŒé¡å‹çš„å¤±æ•—
        if "æ¨£æœ¬ä¸å­˜åœ¨" in error_str or "item not found" in error_str:
            step = "sample_not_found"  # æ¨£æœ¬åœ¨æ•¸æ“šåº«ä¸­ä¸å­˜åœ¨
        else:
            step = "unknown_process"
        if "prefetch" in error_str:
            step = "prefetch"
        elif "fasterq-dump" in error_str or "fastq" in error_str:
            step = "dumping"
        elif "upload" in error_str:
            step = "upload"
        elif "nas" in error_str:
            step = "nas_connect"
        
        progress_mgr.mark_failed(run_id, step, str(e))

        return False
    
    finally:
        # ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œéƒ½ç¢ºä¿æ–·é–‹NASé€£æ¥
        if nas_uploader and nas_uploader.sftp:
            nas_uploader.disconnect()
            print(f"    ğŸ”Œ æ¨£æœ¬ {run_id} çš„ç¨ç«‹NASé€£æ¥å·²é—œé–‰")


# ==================== ä¸»ç¨‹åº ====================


def main():
    print("=" * 80)
    print("ğŸš€ è‡ªå‹•åŒ–ä¸‹è¼‰ã€è§£å£“ã€ä¸Šå‚³ç³»çµ±")
    print("=" * 80)
    print(f"\nç³»çµ±é…ç½®:")
    print(f"  CPUå„ªåŒ–: I7-11ä»£ (8æ ¸16ç·šç¨‹)")
    print(f"  ä¸¦è¡Œæ•¸: {MAX_WORKERS} å€‹æ¨£æœ¬åŒæ™‚è™•ç†")
    print(f"  æ¯å€‹æ¨£æœ¬è§£å£“ç·šç¨‹: {FASTERQ_THREADS}")
    print(f"  ç¸½è§£å£“ç·šç¨‹æ•¸: {MAX_WORKERS * FASTERQ_THREADS}")
    print(f"  ç³»çµ±é ç•™: 2ç·šç¨‹")

    # å‰µå»ºå¿…è¦ç›®éŒ„ï¼ˆæ›´å®‰å…¨çš„æ–¹å¼ï¼‰
    try:
        SRA_TEMP_DIR.mkdir(parents=True, exist_ok=True)
    except FileExistsError:
        # ç›®éŒ„å·²å­˜åœ¨ä½†å¯èƒ½æ˜¯æ›è¼‰é»ï¼Œæª¢æŸ¥æ˜¯å¦å¯å¯«
        if not SRA_TEMP_DIR.exists():
            raise Exception(f"ç„¡æ³•å‰µå»ºç›®éŒ„: {SRA_TEMP_DIR}")
    
    try:
        TMP_DIR.mkdir(parents=True, exist_ok=True)
    except FileExistsError:
        if not TMP_DIR.exists():
            raise Exception(f"ç„¡æ³•å‰µå»ºç›®éŒ„: {TMP_DIR}")
    
    try:
        FASTQ_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    except FileExistsError:
        if not FASTQ_OUTPUT_DIR.exists():
            raise Exception(f"ç„¡æ³•å‰µå»ºç›®éŒ„: {FASTQ_OUTPUT_DIR}")

    # ç§»é™¤ä¸»å‡½æ•¸ä¸­çš„NASé€£æ¥ï¼Œæ”¹ç‚ºåœ¨æ¯å€‹ç·šç¨‹ä¸­ç¨ç«‹å‰µå»º
    # print(f"\nğŸ”Œ æ­£åœ¨é€£æ¥NAS...")
    # nas_uploader = NASUploader(...)
    # if not nas_uploader.connect():
    #     print("âŒ NASé€£æ¥å¤±æ•—ï¼Œç¨‹åºçµ‚æ­¢")
    #     return
    # print("âœ… NASé€£æ¥æˆåŠŸ")

    # åˆå§‹åŒ–é€²åº¦ç®¡ç†
    progress_mgr = ProgressManager()

    # ç²å–ç¼ºå°‘çš„æ¨£æœ¬
    print(f"\nğŸ” æ­£åœ¨æª¢æŸ¥ç¼ºå°‘çš„æ¨£æœ¬...")
    missing_samples = get_missing_samples()

    print(f"\nğŸ“Š çµ±è¨ˆ:")
    print(f"  éœ€è¦ä¸‹è¼‰: {len(missing_samples)} å€‹æ¨£æœ¬")
    print(f"  NASè·¯å¾‘:")
    print(f"    FASTQ: {NAS_CONFIG['fastq_path']}")
    print(f"    SRA: {NAS_CONFIG['sra_path']}")

    if not missing_samples:
        print("\nâœ… æ‰€æœ‰æ¨£æœ¬éƒ½å·²åœ¨NASä¸Šï¼")
        # nas_uploader.disconnect() # No longer needed here
        return

    # ç¢ºèªé–‹å§‹
    print(f"\n" + "=" * 80)
    print(f"æº–å‚™é–‹å§‹ä¸‹è¼‰ {len(missing_samples)} å€‹æ¨£æœ¬")
    print(f"é ä¼°æ™‚é–“: {len(missing_samples) / MAX_WORKERS * 0.5:.1f} - {len(missing_samples) / MAX_WORKERS * 1.5:.1f} å°æ™‚")
    print(f"=" * 80)

    # å¦‚æœåœ¨éäº’å‹•å¼ç’°å¢ƒä¸­ï¼ˆä¾‹å¦‚ Dockerï¼‰ï¼Œå‰‡è·³éç­‰å¾…
    if not sys.stdout.isatty() and os.environ.get("DEBIAN_FRONTEND") == "noninteractive":
        print("\nåœ¨éäº’å‹•å¼ç’°å¢ƒä¸­ï¼Œè‡ªå‹•é–‹å§‹...")
    else:
        input("\næŒ‰Enteré–‹å§‹ï¼Œæˆ–Ctrl+Cå–æ¶ˆ...")

    # é–‹å§‹è™•ç†
    start_time = time.time()
    success_count = 0
    fail_count = 0

    print(f"\nğŸš€ é–‹å§‹è™•ç†...")

    # ä½¿ç”¨ tqdm é€²åº¦æ¢
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # å°‡ progress_mgr å‚³éçµ¦æ¯å€‹ä»»å‹™ï¼Œä¸å†å‚³éå…±äº«çš„ nas_uploader
        futures = {
            executor.submit(download_sample, run_id, progress_mgr): run_id
            for run_id in missing_samples
        }

        # å‰µå»ºé€²åº¦æ¢
        with tqdm(total=len(missing_samples), desc="ç¸½é«”é€²åº¦", unit="æ¨£æœ¬", 
                  ncols=100, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
            
            for future in as_completed(futures):
                run_id = futures[future]
                try:
                    if future.result():
                        success_count += 1
                        pbar.set_postfix({"æˆåŠŸ": success_count, "å¤±æ•—": fail_count})
                    else:
                        fail_count += 1
                        pbar.set_postfix({"æˆåŠŸ": success_count, "å¤±æ•—": fail_count})
                except Exception as e:
                    print(f"\nâŒ åŸ·è¡ŒéŒ¯èª¤ {run_id}: {e}")
                    fail_count += 1
                    pbar.set_postfix({"æˆåŠŸ": success_count, "å¤±æ•—": fail_count})
                
                # æ›´æ–°é€²åº¦æ¢
                pbar.update(1)

    # å®Œæˆ
    elapsed = time.time() - start_time

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ä»»å‹™å®Œæˆ")
    print("=" * 80)
    print(f"ç¸½è€—æ™‚: {elapsed/3600:.2f} å°æ™‚")
    print(f"æˆåŠŸ: {success_count} å€‹")
    print(f"å¤±æ•—: {fail_count} å€‹")

    # é¡¯ç¤ºä¸å­˜åœ¨çš„æ¨£æœ¬åˆ—è¡¨
    if fail_count > 0:
        failed_list = progress_mgr.progress.get("failed", [])
        not_found_samples = [
            f["run_id"] for f in failed_list 
            if f.get("step") == "sample_not_found"
        ]
        
        if not_found_samples:
            print(f"\nâš ï¸  ä»¥ä¸‹ {len(not_found_samples)} å€‹æ¨£æœ¬åœ¨SRAæ•¸æ“šåº«ä¸­ä¸å­˜åœ¨ï¼ˆå¯èƒ½å·²ä¸‹æ¶ï¼‰:")
            for sample in not_found_samples:
                print(f"   - {sample}")
            print(f"\n   é€™äº›æ¨£æœ¬å°‡è¢«è·³éï¼Œæ˜¯æ­£å¸¸ç¾è±¡ã€‚")
        
        # é¡¯ç¤ºå…¶ä»–çœŸæ­£çš„éŒ¯èª¤
        real_errors = [
            f for f in failed_list 
            if f.get("step") != "sample_not_found"
        ]
        
        if real_errors:
            print(f"\nâŒ ä»¥ä¸‹ {len(real_errors)} å€‹æ¨£æœ¬ç™¼ç”ŸçœŸæ­£çš„éŒ¯èª¤:")
            for error in real_errors[:10]:  # æœ€å¤šé¡¯ç¤º10å€‹
                print(f"   - {error['run_id']}: {error.get('step', 'unknown')}")
                print(f"     {error.get('error', '')[:100]}")

    # nas_uploader.disconnect() # No longer needed here


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ¶ä¸­æ–·ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
